{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!python -m spacy download en # one time run",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n\u001b[K     |████████████████████████████████| 11.1MB 4.6MB/s eta 0:00:01    |████▎                           | 1.5MB 1.6MB/s eta 0:00:06\n\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp35-none-any.whl size=11075235 sha256=1049c6441b5eaf041c96d3a5b98a43f2e26b618c07aa68252eb956376ed36ebe\n  Stored in directory: /tmp/pip-ephem-wheel-cache-kohkdq51/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\nSuccessfully built en-core-web-sm\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-2.1.0\n\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\n\u001b[38;5;2m✔ Linking successful\u001b[0m\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/en_core_web_sm -->\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/spacy/data/en\nYou can now load the model via spacy.load('en')\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Run in terminal or command prompt\n# python3 -m spacy download en\n\nimport numpy as np\nimport pandas as pd\nimport re, nltk, spacy, gensim\n\n# Sklearn\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/matplotlib/font_manager.py:281: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = pd.read_json('abstract.json', lines=True)\ndf.head()\n",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The rapid increasing of online information is ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Keywords are very important for any academic p...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Paper presents a survey of methods and approac...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Every year thousands of academic studies are p...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We describe latent Dirichlet allocation (LDA),...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                            abstract\n0  The rapid increasing of online information is ...\n1  Keywords are very important for any academic p...\n2  Paper presents a survey of methods and approac...\n3  Every year thousands of academic studies are p...\n4  We describe latent Dirichlet allocation (LDA),..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.iloc[0:5]",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The rapid increasing of online information is ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Keywords are very important for any academic p...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Paper presents a survey of methods and approac...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Every year thousands of academic studies are p...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>We describe latent Dirichlet allocation (LDA),...</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                            abstract\n0  The rapid increasing of online information is ...\n1  Keywords are very important for any academic p...\n2  Paper presents a survey of methods and approac...\n3  Every year thousands of academic studies are p...\n4  We describe latent Dirichlet allocation (LDA),..."
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "data = df.iloc[0:5].values.tolist()\nprint(data)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[['The rapid increasing of online information is hard to handle.Summaries such as abstracts help us to reduce this problem. Keywords, whichcan be regarded as very short summaries, may help even more. Filteringdocuments by using keywords may save precious time while searching.However, most of the documents do not include keywords. In this paper wepresent a model that extracts keywords from abstracts and titles. This model hasbeen implemented in a prototype system. We have tested our model on a set ofabstracts of Academic papers containing keywords composed by their authors.Results show that keywords extracted from abstracts and titles may be a primarytool for researchers.'], ['Keywords are very important for any academic paper. We propose the Perceptron Training Rule for keyword extraction from titles and abstracts. We present a system for generating keywords which relies on weights of words in a sentence. The system generates keywords from academic research articles by selecting the most relevant keywords. We compare the keywords generated by our system and those generated by cluster analysis to the keywords given by the authors and analyze the results based on full-keyword matches, partial-keyword matches and no-keyword matches.'], ['Paper presents a survey of methods and approaches for keyword extraction task. In addition to the systematization of methods, the paper gathers a comprehensive review of existing research. Related work on keyword extraction is elaborated for supervised and unsupervised methods, with special emphasis on graphbased methods as well as Croatian keyword extraction. Selectivity-based keyword extraction method is proposed as a new unsupervised graph-based keyword extraction method which extracts nodes from a complex network as keyword candidates. The paper provides guidelines for future research and development of new graph-based approaches for keyword extraction.'], ['Every year thousands of academic studies are published all over the world. When researchers search for a topic, they quickly look at abstracts and keywords. In many academic disciplines, the authors write keywords and abstracts in their publications. On the other hand, there are publications of some disciplines, such as social sciences which do not contain keywords and abstracted information. In addition, there may be no abstract or keyword in some of old publications in all disciplines. Search engines for academic publications usually conduct this search by checking keywords, abstracts and titles. The lack of an abstract and a keyword in the publication makes this situation difficult to provide accurate search results and it prevents the researcher to review the publication quickly. This study proposes a method to generate keywords and an abstract from the text that can be used in academic studies. In the previous studies, k-NN and fuzzy CCG methods have been generally used to solve this problem. Nonetheless, the structures of words have not been examined and semantic analysis has not been used for solving this problem. In this study, the sections of the publication are also divided into parts such as the references, the introduction and the methodology. Each section is graded differently so that the word in each section has a different score. Furthermore, NLP methods were used to analyze texts and phrases, removing prepositions and conjunctions. After these processes, the data was used to generate the keyword using TF–IDF. Text generation for abstract is also performed using the TextRank method with this data. Thus, much more successful, truthful and contextually relevant keywords and abstracts are produced. The proposed method was tested on Sobiad Academic Database, which is employed by 72 universities in Turkey, covering more than 250,000 academic publications. Experimental results were measured with precision and F measure, and the results were found to be promising compared to the previous studies, which focused on keyword derivation and abstract generation.'], ['We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.']]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[['the', 'rapid', 'increasing', 'of', 'online', 'information', 'is', 'hard', 'to', 'handle', 'summaries', 'such', 'as', 'abstracts', 'help', 'us', 'to', 'reduce', 'this', 'problem', 'keywords', 'whichcan', 'be', 'regarded', 'as', 'very', 'short', 'summaries', 'may', 'help', 'even', 'more', 'by', 'using', 'keywords', 'may', 'save', 'precious', 'time', 'while', 'searching', 'however', 'most', 'of', 'the', 'documents', 'do', 'not', 'include', 'keywords', 'in', 'this', 'paper', 'wepresent', 'model', 'that', 'extracts', 'keywords', 'from', 'abstracts', 'and', 'titles', 'this', 'model', 'hasbeen', 'implemented', 'in', 'prototype', 'system', 'we', 'have', 'tested', 'our', 'model', 'on', 'set', 'ofabstracts', 'of', 'academic', 'papers', 'containing', 'keywords', 'composed', 'by', 'their', 'authors', 'results', 'show', 'that', 'keywords', 'extracted', 'from', 'abstracts', 'and', 'titles', 'may', 'be', 'primarytool', 'for', 'researchers'], ['keywords', 'are', 'very', 'important', 'for', 'any', 'academic', 'paper', 'we', 'propose', 'the', 'perceptron', 'training', 'rule', 'for', 'keyword', 'extraction', 'from', 'titles', 'and', 'abstracts', 'we', 'present', 'system', 'for', 'generating', 'keywords', 'which', 'relies', 'on', 'weights', 'of', 'words', 'in', 'sentence', 'the', 'system', 'generates', 'keywords', 'from', 'academic', 'research', 'articles', 'by', 'selecting', 'the', 'most', 'relevant', 'keywords', 'we', 'compare', 'the', 'keywords', 'generated', 'by', 'our', 'system', 'and', 'those', 'generated', 'by', 'cluster', 'analysis', 'to', 'the', 'keywords', 'given', 'by', 'the', 'authors', 'and', 'analyze', 'the', 'results', 'based', 'on', 'full', 'keyword', 'matches', 'partial', 'keyword', 'matches', 'and', 'no', 'keyword', 'matches'], ['paper', 'presents', 'survey', 'of', 'methods', 'and', 'approaches', 'for', 'keyword', 'extraction', 'task', 'in', 'addition', 'to', 'the', 'systematization', 'of', 'methods', 'the', 'paper', 'gathers', 'comprehensive', 'review', 'of', 'existing', 'research', 'related', 'work', 'on', 'keyword', 'extraction', 'is', 'elaborated', 'for', 'supervised', 'and', 'unsupervised', 'methods', 'with', 'special', 'emphasis', 'on', 'graphbased', 'methods', 'as', 'well', 'as', 'croatian', 'keyword', 'extraction', 'selectivity', 'based', 'keyword', 'extraction', 'method', 'is', 'proposed', 'as', 'new', 'unsupervised', 'graph', 'based', 'keyword', 'extraction', 'method', 'which', 'extracts', 'nodes', 'from', 'complex', 'network', 'as', 'keyword', 'candidates', 'the', 'paper', 'provides', 'guidelines', 'for', 'future', 'research', 'and', 'development', 'of', 'new', 'graph', 'based', 'approaches', 'for', 'keyword', 'extraction'], ['every', 'year', 'thousands', 'of', 'academic', 'studies', 'are', 'published', 'all', 'over', 'the', 'world', 'when', 'researchers', 'search', 'for', 'topic', 'they', 'quickly', 'look', 'at', 'abstracts', 'and', 'keywords', 'in', 'many', 'academic', 'disciplines', 'the', 'authors', 'write', 'keywords', 'and', 'abstracts', 'in', 'their', 'publications', 'on', 'the', 'other', 'hand', 'there', 'are', 'publications', 'of', 'some', 'disciplines', 'such', 'as', 'social', 'sciences', 'which', 'do', 'not', 'contain', 'keywords', 'and', 'abstracted', 'information', 'in', 'addition', 'there', 'may', 'be', 'no', 'abstract', 'or', 'keyword', 'in', 'some', 'of', 'old', 'publications', 'in', 'all', 'disciplines', 'search', 'engines', 'for', 'academic', 'publications', 'usually', 'conduct', 'this', 'search', 'by', 'checking', 'keywords', 'abstracts', 'and', 'titles', 'the', 'lack', 'of', 'an', 'abstract', 'and', 'keyword', 'in', 'the', 'publication', 'makes', 'this', 'situation', 'difficult', 'to', 'provide', 'accurate', 'search', 'results', 'and', 'it', 'prevents', 'the', 'researcher', 'to', 'review', 'the', 'publication', 'quickly', 'this', 'study', 'proposes', 'method', 'to', 'generate', 'keywords', 'and', 'an', 'abstract', 'from', 'the', 'text', 'that', 'can', 'be', 'used', 'in', 'academic', 'studies', 'in', 'the', 'previous', 'studies', 'nn', 'and', 'fuzzy', 'ccg', 'methods', 'have', 'been', 'generally', 'used', 'to', 'solve', 'this', 'problem', 'nonetheless', 'the', 'structures', 'of', 'words', 'have', 'not', 'been', 'examined', 'and', 'semantic', 'analysis', 'has', 'not', 'been', 'used', 'for', 'solving', 'this', 'problem', 'in', 'this', 'study', 'the', 'sections', 'of', 'the', 'publication', 'are', 'also', 'divided', 'into', 'parts', 'such', 'as', 'the', 'references', 'the', 'introduction', 'and', 'the', 'methodology', 'each', 'section', 'is', 'graded', 'differently', 'so', 'that', 'the', 'word', 'in', 'each', 'section', 'has', 'different', 'score', 'furthermore', 'nlp', 'methods', 'were', 'used', 'to', 'analyze', 'texts', 'and', 'phrases', 'removing', 'prepositions', 'and', 'conjunctions', 'after', 'these', 'processes', 'the', 'data', 'was', 'used', 'to', 'generate', 'the', 'keyword', 'using', 'tf', 'idf', 'text', 'generation', 'for', 'abstract', 'is', 'also', 'performed', 'using', 'the', 'textrank', 'method', 'with', 'this', 'data', 'thus', 'much', 'more', 'successful', 'truthful', 'and', 'contextually', 'relevant', 'keywords', 'and', 'abstracts', 'are', 'produced', 'the', 'proposed', 'method', 'was', 'tested', 'on', 'sobiad', 'academic', 'database', 'which', 'is', 'employed', 'by', 'universities', 'in', 'turkey', 'covering', 'more', 'than', 'academic', 'publications', 'experimental', 'results', 'were', 'measured', 'with', 'precision', 'and', 'measure', 'and', 'the', 'results', 'were', 'found', 'to', 'be', 'promising', 'compared', 'to', 'the', 'previous', 'studies', 'which', 'focused', 'on', 'keyword', 'derivation', 'and', 'abstract', 'generation'], ['we', 'describe', 'latent', 'dirichlet', 'allocation', 'lda', 'generative', 'probabilistic', 'model', 'for', 'collections', 'of', 'discrete', 'data', 'such', 'as', 'text', 'corpora', 'lda', 'is', 'three', 'level', 'hierarchical', 'bayesian', 'model', 'in', 'which', 'each', 'item', 'of', 'collection', 'is', 'modeled', 'as', 'finite', 'mixture', 'over', 'an', 'underlying', 'set', 'of', 'topics', 'each', 'topic', 'is', 'in', 'turn', 'modeled', 'as', 'an', 'infinite', 'mixture', 'over', 'an', 'underlying', 'set', 'of', 'topic', 'probabilities', 'in', 'the', 'context', 'of', 'text', 'modeling', 'the', 'topic', 'probabilities', 'provide', 'an', 'explicit', 'representation', 'of', 'document', 'we', 'present', 'efficient', 'approximate', 'inference', 'techniques', 'based', 'on', 'variational', 'methods', 'and', 'an', 'em', 'algorithm', 'for', 'empirical', 'bayes', 'parameter', 'estimation', 'we', 'report', 'results', 'in', 'document', 'modeling', 'text', 'classification', 'and', 'collaborative', 'filtering', 'comparing', 'to', 'mixture', 'of', 'unigrams', 'model', 'and', 'the', 'probabilistic', 'lsi', 'model']]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n    return texts_out\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# Run in terminal: python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only Noun, Adj, Verb, Adverb\ndata_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['rapid increasing online information be hard handle summary such abstract help reduce problem keyword whichcan be regard very short summary may help even more use keyword may save precious time search however most document do not include keyword paper wepresent model extract keyword abstract title model hasbeen implement prototype system have test model set ofabstract academic paper contain keyword compose author result show keyword extract abstract title may be primarytool researcher', 'keyword be very important academic paper propose perceptron training rule keyword extraction title abstract present system generate keyword rely weight word sentence system generate keyword academic research article select most relevant keyword compare keyword generate system generate cluster analysis keyword give author analyze result base full keyword match partial keyword match keyword match', 'paper present survey method approach keyword extraction task addition systematization method paper gather comprehensive review exist research relate work keyword extraction be elaborate supervised unsupervised method special emphasis graphbase method as well croatian keyword extraction selectivity base keyword extraction method be propose new unsupervised graph base keyword extraction method extract node complex network keyword candidate paper provide guideline future research development new graph base approach keyword extraction', 'year thousand academic study be publish all world when researcher search topic quickly look abstract keyword many academic discipline author write keyword abstract publication other hand there be publication discipline such social science do not contain keyword abstract information addition there may be abstract keyword old publication discipline search engine academic publication usually conduct search check keyword abstract title lack abstract keyword publication make situation difficult provide accurate search result prevent researcher review publication quickly study propose method generate keyword abstract text can be use academic study previous study fuzzy ccg method have be generally use solve problem nonetheless structure word have not be examine semantic analysis have not be use solve problem study section publication be also divide part such reference introduction methodology section be grade differently word section have different score furthermore nlp method be use analyze text phrase remove preposition conjunction process datum be use generate keyword use idf text generation abstract be also perform use textrank method data thus much more successful truthful contextually relevant keyword abstract be produce propose method be test sobiad academic database be employ university turkey cover more academic publication experimental result be measure precision measure result be find be promise compare previous study focus keyword derivation abstract generation', 'describe latent dirichlet allocation lda generative probabilistic model collection discrete datum such text corpora lda be level hierarchical bayesian model item collection be model finite mixture underlying set topic topic be turn model infinite mixture underlying set topic probability context text model topic probability provide explicit representation document present efficient approximate inference technique base variational method algorithm empirical baye paramet estimation report result document modeling text classification collaborative filtering comparing mixture unigram model probabilistic lsi model']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install -U rake_nltk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting rake_nltk\n  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\nRequirement already satisfied, skipping upgrade: nltk in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from rake_nltk) (3.2.1)\nBuilding wheels for collected packages: rake-nltk\n  Building wheel for rake-nltk (setup.py) ... \u001b[?25l|",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from rake_nltk import Rake\nr = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\nr.extract_keywords_from_text(df['abstract'][3])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "list1=[]\nlist1=r.get_word_degrees()\nprint(list1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "vectorizer = CountVectorizer(analyzer='word',       \n                             min_df=1,                        # minimum reqd occurences of a word \n                             stop_words='english',             # remove stop words\n                             lowercase=True,                   # convert all words to lowercase\n                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n                             # max_features=50000,             # max number of uniq words\n                            )\n\ndata_vectorized = vectorizer.fit_transform(data_lemmatized)\n",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Materialize the sparse data\ndata_dense = data_vectorized.todense()\n\n# Compute Sparsicity = Percentage of Non-Zero cells\nprint(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Sparsicity:  24.854368932038835 %\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Build LDA Model\nlda_model = LatentDirichletAllocation(n_topics=6,               # Number of topics\n                                      max_iter=50,               # Max learning iterations\n                                      learning_method='online',   \n                                      random_state=100,          # Random state\n                                      batch_size=1,            # n docs in each learning iter\n                                      evaluate_every = 1,       # compute perplexity every n iters, default: Don't\n                                      n_jobs = -1,               # Use all available CPUs\n                                     )\nlda_output = lda_model.fit_transform(data_vectorized)\n\nprint(lda_model)  # Model attributes",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n  DeprecationWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "LatentDirichletAllocation(batch_size=1, doc_topic_prior=None,\n             evaluate_every=1, learning_decay=0.7,\n             learning_method='online', learning_offset=10.0,\n             max_doc_update_iter=100, max_iter=50, mean_change_tol=0.001,\n             n_components=10, n_jobs=-1, n_topics=6, perp_tol=0.1,\n             random_state=100, topic_word_prior=None,\n             total_samples=1000000.0, verbose=0)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Log Likelyhood: Higher the better\nprint(\"Log Likelihood: \", lda_model.score(data_vectorized))\n\n# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\nprint(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n\n# See model parameters\npprint(lda_model.get_params())",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Log Likelihood:  -2133.1342778809867\nPerplexity:  218.4818013217678\n{'batch_size': 1,\n 'doc_topic_prior': None,\n 'evaluate_every': 1,\n 'learning_decay': 0.7,\n 'learning_method': 'online',\n 'learning_offset': 10.0,\n 'max_doc_update_iter': 100,\n 'max_iter': 50,\n 'mean_change_tol': 0.001,\n 'n_components': 10,\n 'n_jobs': -1,\n 'n_topics': 6,\n 'perp_tol': 0.1,\n 'random_state': 100,\n 'topic_word_prior': None,\n 'total_samples': 1000000.0,\n 'verbose': 0}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Define Search Param\nsearch_params = {'n_components': [10, 15, 20], 'learning_decay': [.5, .7, .9]}\n\n# Init the Model\nlda = LatentDirichletAllocation()\n\n# Init Grid Search Class\nmodel = GridSearchCV(lda, param_grid=search_params)\n\n# Do the Grid Search\nmodel.fit(data_vectorized)",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n  DeprecationWarning)\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 67,
          "data": {
            "text/plain": "GridSearchCV(cv=None, error_score='raise',\n       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n             mean_change_tol=0.001, n_components=10, n_jobs=1,\n             n_topics=None, perp_tol=0.1, random_state=None,\n             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n       fit_params=None, iid=True, n_jobs=1,\n       param_grid={'n_components': [10, 15, 20], 'learning_decay': [0.5, 0.7, 0.9]},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=0)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Best Model\nbest_lda_model = model.best_estimator_\n# Model Parameters\nprint(\"Best Model's Params: \", model.best_params_)\n\n# Log Likelihood Score\nprint(\"Best Log Likelihood Score: \", model.best_score_)\n\n# Perplexity\nprint(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 10}\nBest Log Likelihood Score:  -1562.089940374514\nModel Perplexity:  293.2488409229588\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Create Document - Topic Matrix\nlda_output = best_lda_model.transform(data_vectorized)",
      "execution_count": 69,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n# column names\ntopicnames = [\"Topic\" + str(i) for i in range(10)]\n#print(topicnames)\n\n# index names\ndocnames = [\"Doc\" + str(i) for i in range(len(data))]\n#print(docnames)\n\n# Make the pandas dataframe\ndf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n\n# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic\n\n# Styling\ndef color_green(val):\n    color = 'green' if val > .1 else 'black'\n    return 'color: {col}'.format(col=color)\n\ndef make_bold(val):\n    weight = 700 if val > .1 else 400\n    return 'font-weight: {weight}'.format(weight=weight)\n\n# Apply Style\ndf_document_topics = df_document_topic.head(25).style.applymap(color_green).applymap(make_bold)\ndf_document_topics\n#df_document_topic.Topic0",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 70,
          "data": {
            "text/plain": "<pandas.formats.style.Styler at 0x7fe5b7f2d978>",
            "text/html": "\n        <style  type=\"text/css\" >\n        \n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col0 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col1 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col2 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col3 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col4 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col5 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col6 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col7 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col8 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col9 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col10 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col0 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col1 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col2 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col3 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col4 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col5 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col6 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col7 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col8 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col9 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col10 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col0 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col1 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col2 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col3 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col4 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col5 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col6 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col7 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col8 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col9 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col10 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col0 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col1 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col2 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col3 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col4 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col5 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col6 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col7 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col8 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col9 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col10 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col0 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col1 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col2 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col3 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col4 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col5 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col6 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col7 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col8 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col9 {\n            \n                color:  black;\n            \n                font-weight:  400;\n            \n            }\n        \n            #T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col10 {\n            \n                color:  green;\n            \n                font-weight:  700;\n            \n            }\n        \n        </style>\n\n        <table id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffb\" None>\n        \n\n        <thead>\n            \n            <tr>\n                \n                \n                <th class=\"blank level0\" >\n                  \n                \n                \n                \n                <th class=\"col_heading level0 col0\" colspan=1>\n                  Topic0\n                \n                \n                \n                <th class=\"col_heading level0 col1\" colspan=1>\n                  Topic1\n                \n                \n                \n                <th class=\"col_heading level0 col2\" colspan=1>\n                  Topic2\n                \n                \n                \n                <th class=\"col_heading level0 col3\" colspan=1>\n                  Topic3\n                \n                \n                \n                <th class=\"col_heading level0 col4\" colspan=1>\n                  Topic4\n                \n                \n                \n                <th class=\"col_heading level0 col5\" colspan=1>\n                  Topic5\n                \n                \n                \n                <th class=\"col_heading level0 col6\" colspan=1>\n                  Topic6\n                \n                \n                \n                <th class=\"col_heading level0 col7\" colspan=1>\n                  Topic7\n                \n                \n                \n                <th class=\"col_heading level0 col8\" colspan=1>\n                  Topic8\n                \n                \n                \n                <th class=\"col_heading level0 col9\" colspan=1>\n                  Topic9\n                \n                \n                \n                <th class=\"col_heading level0 col10\" colspan=1>\n                  dominant_topic\n                \n                \n            </tr>\n            \n        </thead>\n        <tbody>\n            \n            <tr>\n                \n                \n                <th id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffb\"\n                 class=\"row_heading level0 row0\" rowspan=1>\n                    Doc0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col0\"\n                 class=\"data row0 col0\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col1\"\n                 class=\"data row0 col1\" >\n                    0.98\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col2\"\n                 class=\"data row0 col2\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col3\"\n                 class=\"data row0 col3\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col4\"\n                 class=\"data row0 col4\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col5\"\n                 class=\"data row0 col5\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col6\"\n                 class=\"data row0 col6\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col7\"\n                 class=\"data row0 col7\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col8\"\n                 class=\"data row0 col8\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col9\"\n                 class=\"data row0 col9\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow0_col10\"\n                 class=\"data row0 col10\" >\n                    1\n                \n                \n            </tr>\n            \n            <tr>\n                \n                \n                <th id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffb\"\n                 class=\"row_heading level0 row1\" rowspan=1>\n                    Doc1\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col0\"\n                 class=\"data row1 col0\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col1\"\n                 class=\"data row1 col1\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col2\"\n                 class=\"data row1 col2\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col3\"\n                 class=\"data row1 col3\" >\n                    0.98\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col4\"\n                 class=\"data row1 col4\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col5\"\n                 class=\"data row1 col5\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col6\"\n                 class=\"data row1 col6\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col7\"\n                 class=\"data row1 col7\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col8\"\n                 class=\"data row1 col8\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col9\"\n                 class=\"data row1 col9\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow1_col10\"\n                 class=\"data row1 col10\" >\n                    3\n                \n                \n            </tr>\n            \n            <tr>\n                \n                \n                <th id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffb\"\n                 class=\"row_heading level0 row2\" rowspan=1>\n                    Doc2\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col0\"\n                 class=\"data row2 col0\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col1\"\n                 class=\"data row2 col1\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col2\"\n                 class=\"data row2 col2\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col3\"\n                 class=\"data row2 col3\" >\n                    0.99\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col4\"\n                 class=\"data row2 col4\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col5\"\n                 class=\"data row2 col5\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col6\"\n                 class=\"data row2 col6\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col7\"\n                 class=\"data row2 col7\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col8\"\n                 class=\"data row2 col8\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col9\"\n                 class=\"data row2 col9\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow2_col10\"\n                 class=\"data row2 col10\" >\n                    3\n                \n                \n            </tr>\n            \n            <tr>\n                \n                \n                <th id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffb\"\n                 class=\"row_heading level0 row3\" rowspan=1>\n                    Doc3\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col0\"\n                 class=\"data row3 col0\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col1\"\n                 class=\"data row3 col1\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col2\"\n                 class=\"data row3 col2\" >\n                    0.99\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col3\"\n                 class=\"data row3 col3\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col4\"\n                 class=\"data row3 col4\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col5\"\n                 class=\"data row3 col5\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col6\"\n                 class=\"data row3 col6\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col7\"\n                 class=\"data row3 col7\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col8\"\n                 class=\"data row3 col8\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col9\"\n                 class=\"data row3 col9\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow3_col10\"\n                 class=\"data row3 col10\" >\n                    2\n                \n                \n            </tr>\n            \n            <tr>\n                \n                \n                <th id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffb\"\n                 class=\"row_heading level0 row4\" rowspan=1>\n                    Doc4\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col0\"\n                 class=\"data row4 col0\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col1\"\n                 class=\"data row4 col1\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col2\"\n                 class=\"data row4 col2\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col3\"\n                 class=\"data row4 col3\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col4\"\n                 class=\"data row4 col4\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col5\"\n                 class=\"data row4 col5\" >\n                    0.99\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col6\"\n                 class=\"data row4 col6\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col7\"\n                 class=\"data row4 col7\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col8\"\n                 class=\"data row4 col8\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col9\"\n                 class=\"data row4 col9\" >\n                    0\n                \n                \n                \n                <td id=\"T_9bece016_c9b2_11e9_a2fb_00155dd4dffbrow4_col10\"\n                 class=\"data row4 col10\" >\n                    5\n                \n                \n            </tr>\n            \n        </tbody>\n        </table>\n        "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\ndf_topic_distribution.columns = ['Topic Num', 'Num Documents']\ndf_topic_distribution",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 71,
          "data": {
            "text/plain": "   Topic Num  Num Documents\n0          3              2\n1          5              1\n2          2              1\n3          1              1",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic Num</th>\n      <th>Num Documents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pyLDAvis.enable_notebook()\npanel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\npanel",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 72,
          "data": {
            "text/plain": "PreparedData(topic_coordinates=            Freq  cluster  topics          x           y\ntopic                                                   \n2      41.034990        1       1  27.143642   64.281883\n3      26.901463        1       2  23.456106   87.020096\n5      17.804630        1       3   7.785783   70.357086\n1      13.512784        1       4  21.598194  109.817017\n7       0.124355        1       5   2.220890  115.873291\n6       0.124355        1       6   5.914620   93.157555\n9       0.124355        1       7 -11.146079   77.676384\n0       0.124355        1       8 -13.747846  100.025269\n4       0.124355        1       9  43.122646   80.131462\n8       0.124355        1      10  40.511948  102.481453, topic_info=     Category       Freq             Term      Total  loglift  logprob\nterm                                                                  \n92    Default  25.000000          keyword  25.000000  30.0000  30.0000\n137   Default   7.000000      publication   7.000000  29.0000  29.0000\n0     Default  11.000000         abstract  11.000000  28.0000  28.0000\n172   Default   5.000000            study   5.000000  27.0000  27.0000\n105   Default   7.000000            model   7.000000  26.0000  26.0000\n60    Default   5.000000       extraction   5.000000  25.0000  25.0000\n195   Default   6.000000              use   6.000000  24.0000  24.0000\n102   Default   9.000000           method   9.000000  23.0000  23.0000\n1     Default   7.000000         academic   7.000000  22.0000  22.0000\n69    Default   5.000000         generate   5.000000  21.0000  21.0000\n181   Default   5.000000             text   5.000000  20.0000  20.0000\n158   Default   4.000000           search   4.000000  19.0000  19.0000\n186   Default   4.000000            topic   4.000000  18.0000  18.0000\n115   Default   4.000000            paper   4.000000  17.0000  17.0000\n12    Default   4.000000             base   4.000000  16.0000  16.0000\n44    Default   2.000000       discipline   2.000000  15.0000  15.0000\n159   Default   2.000000          section   2.000000  14.0000  14.0000\n134   Default   3.000000          propose   3.000000  13.0000  13.0000\n152   Default   5.000000           result   5.000000  12.0000  12.0000\n100   Default   2.000000            match   2.000000  11.0000  11.0000\n150   Default   2.000000         research   2.000000  10.0000  10.0000\n201   Default   2.000000             word   2.000000   9.0000   9.0000\n185   Default   3.000000            title   3.000000   8.0000   8.0000\n151   Default   2.000000       researcher   2.000000   7.0000   7.0000\n130   Default   2.000000          problem   2.000000   6.0000   6.0000\n104   Default   2.000000          mixture   2.000000   5.0000   5.0000\n70    Default   2.000000       generation   2.000000   4.0000   4.0000\n139   Default   2.000000          quickly   2.000000   3.0000   3.0000\n126   Default   2.000000         previous   2.000000   2.0000   2.0000\n169   Default   2.000000            solve   2.000000   1.0000   1.0000\n...       ...        ...              ...        ...      ...      ...\n51    Topic10   0.002483        empirical   1.141332   0.5591  -5.2901\n135   Topic10   0.002358        prototype   1.087170   0.5561  -5.3418\n149   Topic10   0.002453   representation   1.134258   0.5532  -5.3022\n88    Topic10   0.002477         infinite   1.145902   0.5529  -5.2923\n9     Topic10   0.002495      approximate   1.155161   0.5522  -5.2850\n14    Topic10   0.002456         bayesian   1.137147   0.5520  -5.3009\n43    Topic10   0.002479        dirichlet   1.149303   0.5506  -5.2916\n61    Topic10   0.002468        filtering   1.147430   0.5478  -5.2961\n87    Topic10   0.002465        inference   1.152985   0.5418  -5.2973\n98    Topic10   0.002428              lsi   1.136404   0.5411  -5.3125\n107   Topic10   0.002528          network   1.230203   0.5021  -5.2721\n177   Topic10   0.002521  systematization   1.226716   0.5023  -5.2748\n163   Topic10   0.002507         sentence   1.224030   0.4991  -5.2802\n74    Topic10   0.002505        graphbase   1.227489   0.4953  -5.2811\n38    Topic10   0.002557       derivation   1.306499   0.4536  -5.2605\n50    Topic10   0.002494         emphasis   1.216935   0.4994  -5.2856\n125   Topic10   0.002508          prevent   1.292941   0.4448  -5.2797\n189   Topic10   0.002502           turkey   1.297147   0.4390  -5.2823\n49    Topic10   0.002476        elaborate   1.217782   0.4915  -5.2929\n175   Topic10   0.002474       supervised   1.211739   0.4957  -5.2937\n131   Topic10   0.002489          process   1.295567   0.4351  -5.2873\n168   Topic10   0.002481           social   1.295218   0.4322  -5.2905\n95    Topic10   0.002502              lda   1.828249   0.0956  -5.2824\n182   Topic10   0.002470         textrank   1.300990   0.4232  -5.2951\n152   Topic10   0.002530           result   5.040701  -0.9072  -5.2710\n104   Topic10   0.002502          mixture   2.517243  -0.2242  -5.2825\n136   Topic10   0.002501          provide   2.739111  -0.3090  -5.2827\n126   Topic10   0.002494         previous   2.137133  -0.0637  -5.2856\n151   Topic10   0.002492       researcher   2.753979  -0.3181  -5.2865\n0     Topic10   0.002483         abstract  11.455301  -1.7470  -5.2900\n\n[547 rows x 6 columns], token_table=      Topic      Freq            Term\nterm                                 \n0         1  0.698367        abstract\n0         2  0.087296        abstract\n0         4  0.174592        abstract\n1         1  0.657576        academic\n1         2  0.263030        academic\n1         4  0.131515        academic\n3         1  0.484283        addition\n3         2  0.484283        addition\n4         3  0.864093       algorithm\n5         3  0.872705      allocation\n6         1  0.485691        analysis\n6         2  0.485691        analysis\n7         1  0.484126         analyze\n7         2  0.484126         analyze\n8         2  1.001243        approach\n9         3  0.865680     approximate\n10        2  0.821691         article\n11        1  0.373023          author\n11        2  0.373023          author\n11        4  0.373023          author\n12        2  0.714560            base\n12        3  0.238187            base\n13        3  0.860995            baye\n14        3  0.879393        bayesian\n15        2  0.816916       candidate\n16        1  0.761960             ccg\n17        1  0.774418           check\n18        3  0.877940  classification\n19        2  0.822329         cluster\n20        3  0.863420   collaborative\n...     ...       ...             ...\n181       1  0.596841            text\n181       3  0.397894            text\n182       1  0.768645        textrank\n183       1  0.774821        thousand\n184       4  0.915831            time\n185       1  0.300660           title\n185       2  0.300660           title\n185       4  0.300660           title\n186       1  0.247653           topic\n186       3  0.742960           topic\n187       2  0.820386        training\n188       1  0.777063        truthful\n189       1  0.770923          turkey\n190       3  0.875520            turn\n191       3  0.544159      underlying\n192       3  0.861013         unigram\n193       1  0.768673      university\n194       2  1.014022    unsupervised\n195       1  0.866385             use\n195       4  0.144398             use\n196       1  0.770100         usually\n197       3  0.880265     variational\n198       2  0.810436          weight\n199       4  0.922333       wepresent\n200       4  0.931437        whichcan\n201       1  0.686501            word\n201       2  0.343251            word\n202       2  0.816256            work\n203       1  0.778013           world\n205       1  0.772819            year\n\n[234 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 4, 6, 2, 8, 7, 10, 1, 5, 9])",
            "text/html": "\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el1471406246136879688989902436\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el1471406246136879688989902436_data = {\"R\": 30, \"mdsDat\": {\"y\": [64.2818832397461, 87.02009582519531, 70.35708618164062, 109.8170166015625, 115.873291015625, 93.15755462646484, 77.67638397216797, 100.0252685546875, 80.13146209716797, 102.48145294189453], \"x\": [27.14364242553711, 23.456106185913086, 7.785782814025879, 21.598194122314453, 2.2208902835845947, 5.9146199226379395, -11.146079063415527, -13.747845649719238, 43.12264633178711, 40.51194763183594], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"Freq\": [41.03499041165668, 26.901463321826995, 17.80463048396033, 13.512783648674192, 0.12435538926380922, 0.12435535245908277, 0.1243553504839146, 0.12435534852580589, 0.12435534691313678, 0.12435534623604985], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 4, 6, 2, 8, 7, 10, 1, 5, 9], \"token.table\": {\"Topic\": [1, 2, 4, 1, 2, 4, 1, 2, 3, 3, 1, 2, 1, 2, 2, 3, 2, 1, 2, 4, 2, 3, 3, 3, 2, 1, 1, 3, 2, 3, 3, 1, 2, 3, 2, 4, 2, 1, 1, 4, 3, 3, 2, 1, 1, 1, 3, 1, 2, 1, 1, 1, 3, 1, 3, 1, 3, 4, 3, 2, 2, 3, 1, 1, 3, 1, 2, 1, 3, 2, 4, 2, 3, 3, 1, 1, 2, 1, 2, 1, 2, 1, 3, 1, 2, 2, 2, 1, 4, 4, 4, 4, 3, 1, 4, 2, 4, 4, 3, 3, 1, 4, 1, 3, 1, 2, 4, 1, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 1, 3, 3, 4, 3, 2, 2, 1, 2, 1, 4, 1, 4, 2, 4, 3, 2, 2, 4, 1, 1, 2, 3, 1, 1, 4, 3, 3, 1, 4, 1, 1, 1, 1, 2, 4, 1, 2, 3, 1, 1, 4, 4, 4, 2, 2, 1, 3, 3, 2, 1, 4, 1, 2, 3, 4, 2, 4, 1, 1, 1, 4, 1, 2, 2, 1, 2, 3, 4, 4, 1, 1, 1, 1, 2, 1, 1, 1, 4, 2, 2, 2, 2, 3, 1, 4, 1, 3, 1, 1, 4, 1, 2, 4, 1, 3, 2, 1, 1, 3, 3, 3, 1, 2, 1, 4, 1, 3, 2, 4, 4, 1, 2, 2, 1, 1], \"Term\": [\"abstract\", \"abstract\", \"abstract\", \"academic\", \"academic\", \"academic\", \"addition\", \"addition\", \"algorithm\", \"allocation\", \"analysis\", \"analysis\", \"analyze\", \"analyze\", \"approach\", \"approximate\", \"article\", \"author\", \"author\", \"author\", \"base\", \"base\", \"baye\", \"bayesian\", \"candidate\", \"ccg\", \"check\", \"classification\", \"cluster\", \"collaborative\", \"collection\", \"compare\", \"compare\", \"comparing\", \"complex\", \"compose\", \"comprehensive\", \"conduct\", \"contain\", \"contain\", \"context\", \"corpora\", \"croatian\", \"data\", \"database\", \"datum\", \"datum\", \"derivation\", \"development\", \"different\", \"differently\", \"difficult\", \"dirichlet\", \"discipline\", \"discrete\", \"divide\", \"document\", \"document\", \"efficient\", \"elaborate\", \"emphasis\", \"empirical\", \"employ\", \"engine\", \"estimation\", \"examine\", \"exist\", \"experimental\", \"explicit\", \"extract\", \"extract\", \"extraction\", \"filtering\", \"finite\", \"focus\", \"furthermore\", \"future\", \"fuzzy\", \"gather\", \"generate\", \"generate\", \"generation\", \"generative\", \"grade\", \"graph\", \"graphbase\", \"guideline\", \"hand\", \"handle\", \"hard\", \"hasbeen\", \"help\", \"hierarchical\", \"idf\", \"implement\", \"important\", \"include\", \"increasing\", \"inference\", \"infinite\", \"information\", \"information\", \"introduction\", \"item\", \"keyword\", \"keyword\", \"keyword\", \"lack\", \"latent\", \"lda\", \"level\", \"lsi\", \"make\", \"match\", \"measure\", \"method\", \"method\", \"method\", \"methodology\", \"mixture\", \"model\", \"model\", \"modeling\", \"network\", \"new\", \"nlp\", \"node\", \"nonetheless\", \"ofabstract\", \"old\", \"online\", \"paper\", \"paper\", \"paramet\", \"partial\", \"perceptron\", \"precious\", \"precision\", \"preposition\", \"present\", \"present\", \"prevent\", \"previous\", \"primarytool\", \"probabilistic\", \"probability\", \"problem\", \"problem\", \"process\", \"produce\", \"promise\", \"propose\", \"propose\", \"prototype\", \"provide\", \"provide\", \"provide\", \"publication\", \"quickly\", \"rapid\", \"reduce\", \"regard\", \"relate\", \"rely\", \"remove\", \"report\", \"representation\", \"research\", \"researcher\", \"researcher\", \"result\", \"result\", \"result\", \"result\", \"rule\", \"save\", \"science\", \"score\", \"search\", \"search\", \"section\", \"select\", \"selectivity\", \"semantic\", \"sentence\", \"set\", \"set\", \"short\", \"situation\", \"sobiad\", \"social\", \"solve\", \"special\", \"structure\", \"study\", \"successful\", \"summary\", \"supervised\", \"survey\", \"systematization\", \"task\", \"technique\", \"test\", \"test\", \"text\", \"text\", \"textrank\", \"thousand\", \"time\", \"title\", \"title\", \"title\", \"topic\", \"topic\", \"training\", \"truthful\", \"turkey\", \"turn\", \"underlying\", \"unigram\", \"university\", \"unsupervised\", \"use\", \"use\", \"usually\", \"variational\", \"weight\", \"wepresent\", \"whichcan\", \"word\", \"word\", \"work\", \"world\", \"year\"], \"Freq\": [0.6983666105854068, 0.08729582632317585, 0.1745916526463517, 0.6575756685760312, 0.26303026743041247, 0.13151513371520623, 0.4842825971913665, 0.4842825971913665, 0.8640928825228938, 0.8727051017763544, 0.48569148551463553, 0.48569148551463553, 0.4841263526277705, 0.4841263526277705, 1.0012429344204032, 0.8656802394853534, 0.8216906075015231, 0.37302327682500974, 0.37302327682500974, 0.37302327682500974, 0.7145602077769273, 0.23818673592564243, 0.8609952705551519, 0.8793934308847001, 0.81691550204416, 0.7619597004329705, 0.7744181385894461, 0.8779395246857804, 0.822329370276968, 0.8634204106653308, 0.5420335658305296, 0.48518084997771205, 0.48518084997771205, 0.8658866751379093, 0.8106792108000409, 0.9282241782523992, 0.814995104497628, 0.7685282181769719, 0.5178586158460301, 0.5178586158460301, 0.8776417783221231, 0.8692360384103016, 0.8147188737946426, 0.7708202417200282, 0.7691392353210044, 0.5062568748379278, 0.5062568748379278, 0.7654044958447332, 0.8226002234834816, 0.7821959142825036, 0.7694022180413469, 0.7645994823315739, 0.8700926228070048, 1.0069760607772005, 0.8787243292169553, 0.7681695234869791, 0.40795241592799253, 0.40795241592799253, 0.866690899318375, 0.8211648389817262, 0.821736882982243, 0.8761693824011246, 0.7666367015989549, 0.7741372288848744, 0.8742225828620025, 0.7729557918766313, 0.8176292141591615, 0.7830089866567898, 0.8744780624650763, 0.40385706909224023, 0.40385706909224023, 0.8616395678656832, 0.8715125844383391, 0.8683222199487669, 0.765918670925643, 0.7631201859647887, 0.8155589777292344, 0.7673090413341221, 0.8188771954950743, 0.3854899470648384, 0.5782349205972577, 0.937415639781413, 0.87457036371057, 0.7783031615516034, 1.0076313854035932, 0.8146715747532122, 0.8208536913337854, 0.7717070596519542, 0.9271299674211686, 0.9207685176696987, 0.9181368748092881, 0.5823208817518672, 0.878017450416475, 0.7838505092846444, 0.9229882599374951, 0.823690479894851, 0.9195596652791291, 0.9261965946667805, 0.86731379811493, 0.8726752049310531, 0.517511684299202, 0.517511684299202, 0.7689049745068779, 0.8732361708536946, 0.31296333252242425, 0.5085654153489394, 0.15648166626121213, 0.7724210622717435, 0.873384097013431, 0.5469714265870067, 0.8601742158084068, 0.879969016615242, 0.764059255438441, 0.7276916068220439, 0.9365064575381805, 0.4040272112787442, 0.5050340140984303, 0.10100680281968605, 0.7648586566193878, 0.7945200784372649, 0.7021282502402153, 0.2808513000960861, 0.8739309054281665, 0.8128741496681771, 1.0065505806868402, 0.7851482865741107, 0.8098827006970502, 0.7722063192175777, 0.9171572677707438, 0.7706922295172287, 0.9172318311986017, 0.6283713894486926, 0.20945712981623088, 0.8694455619833624, 0.8131911517022968, 0.8224506758875705, 0.9230926544409916, 0.7729020200271979, 0.772645705238699, 0.7482839502688567, 0.3741419751344284, 0.7734304726549214, 0.9358333892676601, 0.9175446840020677, 0.5430897274925801, 0.5477097899083945, 0.7297668195937392, 0.3648834097968696, 0.7718629812180318, 0.7796320325707146, 0.781998950078352, 0.5464114617355823, 0.5464114617355823, 0.9198196087466566, 0.36508196546943633, 0.36508196546943633, 0.36508196546943633, 0.9811801364600892, 0.9285342124381974, 0.9143235516674374, 0.9293938916285703, 0.9231284983969322, 0.8187619958447722, 0.8249321785907289, 0.772845922494461, 0.8658904489664839, 0.8816339358047052, 0.725435025161519, 0.7262218930341418, 0.3631109465170709, 0.5951553178043691, 0.1983851059347897, 0.1983851059347897, 0.1983851059347897, 0.8231525640698129, 0.9288760027475563, 0.7686815020721696, 0.7706923057012648, 0.6762282522630744, 0.22540941742102477, 1.0121947794091606, 0.8105126871817494, 0.8243842644781586, 0.7776594731836102, 0.8169734134957822, 0.4069125255175341, 0.4069125255175341, 0.9232404355442332, 0.7684920413128955, 0.7732515351831428, 0.7720709391006454, 0.9425306425967461, 0.8172473386822595, 0.7660651743633353, 0.9127148861493631, 0.7657469549811234, 0.586888374417385, 0.8252604348741049, 0.8115030320609545, 0.8151842874373483, 0.8150966928869379, 0.8722879954956066, 0.5222119065610874, 0.5222119065610874, 0.596840888780985, 0.39789392585398997, 0.7686453548477995, 0.7748206103803664, 0.9158309863174631, 0.30065967240262226, 0.30065967240262226, 0.30065967240262226, 0.24765333233010098, 0.7429599969903029, 0.8203859816416104, 0.7770630404706488, 0.7709229447070836, 0.875520178596119, 0.5441586842531354, 0.8610126374834385, 0.7686731811580133, 1.0140224100572461, 0.8663852444548785, 0.14439754074247976, 0.7700996688758506, 0.8802647586542066, 0.8104364352007758, 0.9223329536799505, 0.9314367825920883, 0.6865011676881332, 0.3432505838440666, 0.8162561023899816, 0.7780125108160247, 0.7728190561966167]}, \"lambda.step\": 0.01, \"tinfo\": {\"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8416, 0.8263, 0.7711, 0.7694, 0.7417, 0.7233, 0.7219, 0.7203, 0.7195, 0.7167, 0.6458, 0.6046, 0.6032, 0.6031, 0.6019, 0.6006, 0.6004, 0.5998, 0.5997, 0.5996, 0.5996, 0.5983, 0.5977, 0.5975, 0.5973, 0.5973, 0.5969, 0.5967, 0.5964, 0.5961, 0.5896, 0.4945, 0.4679, 0.4634, 0.4101, 0.595, 0.2473, 0.0564, 0.2383, -0.2113, 0.1816, 0.5923, -0.1658, 0.5907, 0.5932, 0.5939, 0.1377, 0.1374, 1.2528, 1.1807, 1.1789, 1.1285, 1.1244, 1.1202, 1.1199, 1.0312, 0.9952, 0.9935, 0.9907, 0.99, 0.9896, 0.989, 0.9889, 0.9889, 0.9887, 0.9884, 0.9876, 0.9867, 0.9864, 0.9864, 0.9862, 0.9853, 0.9853, 0.9847, 0.9846, 0.9839, 0.9837, 0.983, 0.904, 0.6442, 0.8205, 0.8296, 0.5664, 0.5122, 0.9816, 0.9796, 0.9789, 0.9809, -0.2219, 0.9779, 0.9772, 0.1239, -0.0119, 0.4635, -1.2501, -0.4292, 1.5704, 1.5114, 1.5098, 1.5075, 1.505, 1.5026, 1.3787, 1.3741, 1.3648, 1.3616, 1.3603, 1.3596, 1.3592, 1.3581, 1.3579, 1.3575, 1.3563, 1.3563, 1.3563, 1.3555, 1.3554, 1.3551, 1.355, 1.3543, 1.3538, 1.3534, 1.3521, 1.352, 1.352, 1.3515, 1.3489, 1.2195, 1.2191, 0.8821, 1.3488, 1.3505, 1.3494, 1.3445, 1.35, 1.3479, 0.8153, -0.7932, 0.063, 0.4881, 0.5088, 1.767, 1.7587, 1.6063, 1.6052, 1.6014, 1.6004, 1.5981, 1.5976, 1.5968, 1.5959, 1.5959, 1.5952, 1.5932, 1.5931, 1.5895, 1.5889, 1.5881, 1.588, 1.5875, 1.5849, 1.5836, 1.583, 1.3918, 1.1023, 1.0259, 1.0253, 1.0197, 0.7782, 0.7766, 0.7409, 0.721, 0.1081, 0.2414, 0.6661, 0.6928, 0.0666, 0.6101, 0.5941, 0.5908, 0.5781, 0.5638, 0.5613, 0.5564, 0.5561, 0.5533, 0.5512, 0.5503, 0.5478, 0.5473, 0.5455, 0.5447, 0.5447, 0.5447, 0.5436, 0.5382, 0.533, 0.5298, 0.5287, 0.5287, 0.5272, 0.5264, 0.5262, 0.5227, 0.5185, 0.5162, 0.5134, 0.5095, 0.4721, 0.4699, 0.4523, 0.4536, 0.4433, 0.4743, 0.4358, 0.4316, 0.4272, 0.4237, 0.4293, 0.4314, 0.1552, 0.0922, 0.4218, 0.4127, -1.0596, -1.0865, -1.079, -0.0364, -0.8781, -1.5516, 0.0372, -0.6271, -0.8389, -1.2225, -0.8439, -1.4944, -0.0441, -0.2032, -2.431, -0.018, -0.9235, -0.7792, 0.6781, 0.632, 0.632, 0.5935, 0.5911, 0.5862, 0.5783, 0.5768, 0.5715, 0.5715, 0.5696, 0.5677, 0.5659, 0.5608, 0.559, 0.5575, 0.5565, 0.5445, 0.5445, 0.5373, 0.5365, 0.5341, 0.5338, 0.5324, 0.5311, 0.5301, 0.5297, 0.5266, 0.5262, 0.526, 0.521, 0.524, 0.4961, 0.5107, 0.4995, 0.4961, 0.4571, 0.4554, 0.4642, 0.482, 0.4501, 0.4487, 0.4462, 0.4827, 0.435, 0.4313, 0.4763, 0.4702, -0.2463, 0.4256, -0.4756, -0.0198, 0.0379, -0.0583, -0.024, -0.064, -1.2249, 0.6132, 0.6016, 0.5993, 0.5972, 0.5962, 0.5951, 0.5896, 0.5812, 0.5808, 0.5775, 0.576, 0.5701, 0.5679, 0.5632, 0.5631, 0.5619, 0.5595, 0.558, 0.5568, 0.556, 0.5508, 0.547, 0.5468, 0.5459, 0.5445, 0.5438, 0.5387, 0.5386, 0.5381, 0.5365, 0.5285, 0.5141, 0.5044, 0.5241, 0.4988, 0.4954, 0.4577, 0.4639, 0.4451, 0.4374, 0.4839, 0.4363, 0.4337, 0.4364, 0.4302, 0.4307, 0.4344, 0.4291, 0.425, 0.4286, 0.4172, 0.4217, -0.3782, 0.162, -0.4994, 0.0026, 0.0324, 0.0036, -0.3197, 0.6349, 0.63, 0.6285, 0.6125, 0.6027, 0.5946, 0.5872, 0.5856, 0.5805, 0.5799, 0.5765, 0.5682, 0.5651, 0.5638, 0.5632, 0.5611, 0.5605, 0.5596, 0.5591, 0.5527, 0.5515, 0.5512, 0.5481, 0.5478, 0.5472, 0.5465, 0.5341, 0.5338, 0.5328, 0.5319, 0.5313, 0.5301, 0.5295, 0.5136, 0.509, 0.5259, 0.4875, 0.4892, 0.4504, 0.4573, 0.4447, 0.4409, 0.4401, 0.441, 0.4351, 0.4306, 0.4361, 0.4317, -0.2708, 0.4436, 0.0276, -0.2996, 0.1549, 0.0871, -0.7971, -0.7053, 0.6414, 0.6262, 0.6251, 0.6237, 0.6225, 0.6125, 0.6077, 0.6039, 0.6033, 0.593, 0.5863, 0.5838, 0.5766, 0.5685, 0.5606, 0.5558, 0.5516, 0.5505, 0.5498, 0.5471, 0.5463, 0.5438, 0.5433, 0.5418, 0.5403, 0.5357, 0.5346, 0.5336, 0.5334, 0.5322, 0.5219, 0.5018, 0.4935, 0.4871, 0.4977, 0.5249, 0.4952, 0.4541, 0.4673, 0.4912, 0.4883, 0.4417, 0.4434, 0.4397, 0.4866, 0.436, 0.4345, 0.1237, 0.1062, 0.432, -0.2214, -0.2864, 0.4262, 0.6324, 0.6269, 0.622, 0.6193, 0.618, 0.6033, 0.6017, 0.6014, 0.6013, 0.5982, 0.5939, 0.5927, 0.5925, 0.5892, 0.5822, 0.5743, 0.5717, 0.5645, 0.5644, 0.5598, 0.5591, 0.5561, 0.5532, 0.5529, 0.5522, 0.552, 0.5506, 0.5478, 0.5418, 0.5411, 0.5021, 0.5023, 0.4991, 0.4953, 0.4536, 0.4994, 0.4448, 0.439, 0.4915, 0.4957, 0.4351, 0.4322, 0.0956, 0.4232, -0.9072, -0.2242, -0.309, -0.0637, -0.3181, -1.747], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.1749, -3.4543, -4.1238, -4.1204, -3.3045, -4.5058, -4.5008, -4.5033, -4.5024, -4.4975, -3.8458, -5.1253, -5.1336, -5.1348, -5.1203, -5.1218, -5.1207, -5.136, -5.1399, -5.1213, -5.1239, -5.1148, -5.1261, -5.1262, -5.1216, -5.1382, -5.1293, -5.1337, -5.1275, -5.1271, -2.9534, -3.4583, -4.5053, -4.5049, -4.502, -5.1231, -4.1194, -3.6325, -4.1255, -2.9516, -4.5023, -5.1188, -4.5009, -5.1188, -5.1202, -5.1209, -5.1183, -5.1207, -2.9703, -3.7897, -3.7884, -4.1737, -4.1715, -4.1746, -4.1696, -3.5156, -4.7892, -4.7899, -4.7986, -4.787, -4.792, -4.7999, -4.7994, -4.7962, -4.7963, -4.7977, -4.7964, -4.7934, -4.801, -4.7996, -4.7996, -4.7915, -4.794, -4.7918, -4.8022, -4.7932, -4.7955, -4.7984, -3.5142, -2.0961, -3.5145, -4.1687, -3.1224, -4.1717, -4.7931, -4.7919, -4.7917, -4.7933, -4.1747, -4.7935, -4.7954, -4.7883, -4.7916, -4.7928, -4.7931, -4.7931, -3.4878, -3.868, -3.8631, -3.8634, -3.864, -3.8755, -3.207, -4.4795, -4.4837, -4.4923, -4.4906, -4.4933, -4.4962, -4.494, -4.4934, -4.4895, -4.495, -4.4903, -4.4745, -4.4937, -4.4894, -4.4923, -4.4923, -4.4775, -4.492, -4.4812, -4.4894, -4.4851, -4.494, -4.4924, -2.6695, -3.8628, -3.8657, -3.4846, -4.4865, -4.4885, -4.489, -4.4873, -4.4902, -4.4895, -4.4854, -4.482, -4.4837, -4.4856, -4.4895, -3.6815, -3.6819, -4.3014, -4.3052, -4.3068, -4.3011, -4.3088, -4.2999, -4.301, -4.3053, -4.3055, -4.3095, -4.3126, -4.303, -4.3056, -4.3125, -4.3108, -4.3073, -4.313, -4.3101, -4.3082, -4.3105, -3.6829, -3.6773, -4.3058, -4.2974, -4.3037, -4.3041, -4.3082, -3.6773, -3.2973, -2.6322, -3.3016, -4.3022, -4.3025, -4.2972, -5.2392, -5.2573, -5.2544, -5.3276, -5.2828, -5.3337, -5.2949, -5.3414, -5.3416, -5.357, -5.3415, -5.3534, -5.294, -5.3523, -5.3513, -5.2387, -5.3001, -5.3553, -5.3694, -5.3624, -5.3162, -5.3171, -5.3782, -5.3742, -5.3291, -5.2578, -5.3708, -5.3336, -5.321, -5.3183, -5.3307, -5.2537, -5.2555, -5.266, -5.2691, -5.275, -5.3131, -5.295, -5.2976, -5.2957, -5.2965, -5.3032, -5.3042, -5.2229, -5.2085, -5.3045, -5.3054, -5.0779, -5.103, -5.1252, -5.2505, -5.1588, -5.0946, -5.265, -5.2128, -5.2028, -5.1753, -5.2106, -5.1833, -5.2677, -5.2615, -5.1713, -5.274, -5.2585, -5.2708, -5.2194, -5.2738, -5.2693, -5.3054, -5.3008, -5.2693, -5.3177, -5.268, -5.3354, -5.3238, -5.3351, -5.3337, -5.3423, -5.2709, -5.336, -5.3501, -5.2809, -5.3028, -5.3571, -5.3731, -5.3147, -5.3198, -5.3046, -5.3205, -5.3063, -5.3634, -5.3177, -5.2609, -5.3251, -5.3225, -5.2496, -5.3078, -5.2172, -5.2689, -5.2776, -5.2809, -5.2616, -5.2657, -5.2737, -5.2896, -5.2736, -5.2756, -5.2792, -5.2989, -5.2875, -5.2896, -5.3012, -5.301, -5.2147, -5.2973, -5.2552, -5.2762, -5.2848, -5.282, -5.2832, -5.286, -5.2712, -5.2882, -5.2919, -5.3089, -5.2988, -5.3107, -5.2462, -5.2589, -5.3246, -5.2662, -5.3176, -5.2557, -5.2811, -5.2736, -5.286, -5.3382, -5.337, -5.3452, -5.3435, -5.3438, -5.3544, -5.3446, -5.3506, -5.2979, -5.3555, -5.3505, -5.3071, -5.3532, -5.308, -5.3042, -5.3164, -5.3174, -5.2718, -5.277, -5.3133, -5.2787, -5.2761, -5.2519, -5.2686, -5.2751, -5.2774, -5.3001, -5.2819, -5.282, -5.2881, -5.289, -5.2903, -5.291, -5.2938, -5.2939, -5.2957, -5.2951, -5.2964, -5.268, -5.2865, -5.279, -5.2869, -5.2903, -5.2922, -5.2935, -5.2602, -5.2747, -5.2772, -5.2881, -5.2988, -5.3123, -5.3205, -5.3098, -5.313, -5.264, -5.3195, -5.2776, -5.2823, -5.2875, -5.3287, -5.3471, -5.337, -5.2817, -5.3422, -5.3027, -5.297, -5.2948, -5.3533, -5.2975, -5.3542, -5.2853, -5.3608, -5.3766, -5.366, -5.3203, -5.3004, -5.3147, -5.3079, -5.2648, -5.2702, -5.3086, -5.237, -5.285, -5.2689, -5.2752, -5.2703, -5.2756, -5.2776, -5.2848, -5.2835, -5.2839, -5.2869, -5.2921, -5.238, -5.2943, -5.2683, -5.2728, -5.2858, -5.2858, -5.2887, -5.291, -5.2564, -5.2807, -5.2825, -5.2777, -5.2879, -5.2922, -5.2937, -5.2921, -5.2917, -5.2989, -5.2676, -5.3115, -5.2743, -5.2668, -5.337, -5.3392, -5.3472, -5.2886, -5.356, -5.3464, -5.355, -5.3568, -5.3102, -5.3034, -5.3613, -5.3103, -5.3124, -5.2981, -5.3179, -5.317, -5.2643, -5.2219, -5.2246, -5.2339, -5.2736, -5.3204, -5.2916, -5.2555, -5.2684, -5.2946, -5.2932, -5.2706, -5.2749, -5.2748, -5.2926, -5.2852, -5.2879, -5.2544, -5.2732, -5.2908, -5.2797, -5.2847, -5.292, -5.2665, -5.2681, -5.2785, -5.2883, -5.2769, -5.3014, -5.2943, -5.3002, -5.2962, -5.3122, -5.3075, -5.3142, -5.3088, -5.319, -5.269, -5.321, -5.3296, -5.3413, -5.2823, -5.2862, -5.2901, -5.3418, -5.3022, -5.2923, -5.285, -5.3009, -5.2916, -5.2961, -5.2973, -5.3125, -5.2721, -5.2748, -5.2802, -5.2811, -5.2605, -5.2856, -5.2797, -5.2823, -5.2929, -5.2937, -5.2873, -5.2905, -5.2824, -5.2951, -5.271, -5.2825, -5.2827, -5.2856, -5.2865, -5.29], \"Total\": [25.0, 7.0, 11.0, 5.0, 7.0, 5.0, 6.0, 9.0, 7.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 2.0, 2.0, 3.0, 5.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 7.134265910900587, 5.478161993275263, 2.9638564247003556, 2.979216802517183, 6.925325700549233, 2.121946926298165, 2.13559659295618, 2.1335253169729076, 2.137132552585143, 2.1539324811180487, 4.436371875857243, 1.2859098802026603, 1.2771245503448125, 1.27575346083862, 1.2958284979937953, 1.2955667318336197, 1.297319330598499, 1.278452088205146, 1.2736447587033093, 1.2975348157154933, 1.294254265855348, 1.3078742833445223, 1.2938250568484873, 1.293918969996464, 1.3001547107171625, 1.2787740954125393, 1.290621321377978, 1.2853263747020494, 1.2937350499336273, 1.2946306734036113, 11.455301382885398, 7.6036876650673735, 2.7406014446003435, 2.7539792165229784, 2.9133235224278144, 1.3011883966630438, 5.026465271384701, 9.900323266197885, 5.040700990570863, 25.562099992742088, 3.660245328030534, 1.3104095768816961, 5.1882027410266165, 1.3124053666247273, 1.3074311068399456, 1.3056216514365167, 2.065576464846706, 2.061087118434162, 5.8028904271251225, 2.748417023434348, 2.7569664141246797, 1.9723429977125366, 1.9848528231372298, 1.9869841003273372, 1.9975172170955238, 4.198386598287244, 1.2177822923349175, 1.2189384294438796, 1.211738692104574, 1.2268483098099259, 1.2211843308146137, 1.2122208660939229, 1.2130265497401354, 1.216934545241301, 1.2170030798339706, 1.2156573405308353, 1.218243897246931, 1.2230482750403997, 1.2140482674118753, 1.2158783855588935, 1.2160577454058232, 1.2270012353220343, 1.224030039999781, 1.2274172504957326, 1.2148416267524236, 1.2267164804459703, 1.2241168119563277, 1.2213561512075708, 4.774246648358827, 25.562099992742088, 5.1882027410266165, 2.6727821694978284, 9.900323266197885, 3.660245328030534, 1.2297231689088675, 1.2335335440674773, 1.2347467097881206, 1.230202732376481, 7.6036876650673735, 1.2337869792971667, 1.2322812860727388, 2.9133235224278144, 3.326019721929552, 2.064910045910333, 11.455301382885398, 5.040700990570863, 2.5172428668307334, 1.8257844179985387, 1.8376992390969786, 1.8413163596685085, 1.8449041960487307, 1.8282490663905457, 4.037902460634305, 1.1364036473084602, 1.142178129581756, 1.136021850436056, 1.139417043149201, 1.1371474528686956, 1.1342576089556455, 1.1380133299496957, 1.1390306187181922, 1.1438734477965913, 1.138929527568802, 1.144255219478785, 1.1625551912878282, 1.141331824743202, 1.1464103658010694, 1.1434185761306448, 1.1435392640738047, 1.1614231388319605, 1.14516557304581, 1.1581843417732323, 1.150158266055001, 1.1551609409433898, 1.1449716149166635, 1.1474303617135566, 7.121206130488806, 2.4575306418207306, 2.451266277527106, 5.026465271384701, 1.1572829961060411, 1.1529852311510065, 1.1538138923420893, 1.1614465656184392, 1.1516462172982311, 1.1548855395433018, 1.9752818177928708, 9.900323266197885, 4.198386598287244, 2.7391109246225347, 2.6727821694978284, 1.7039015315181847, 1.7172662553188502, 1.0765699587911233, 1.073610167312813, 1.0759700585590326, 1.0831414672717605, 1.0773259557650563, 1.0874770150955386, 1.0871696911991222, 1.0834373993746358, 1.083272807346496, 1.0796843842421726, 1.0785974298528187, 1.0891622234513958, 1.0902369128350464, 1.083314871144308, 1.0860492955719447, 1.0898651776154222, 1.0842071683660128, 1.090325547362902, 1.0937047374272661, 1.091904527079804, 2.4761235509575834, 3.326019721929552, 1.9149314434159148, 1.9323235210701928, 1.9310289901545643, 2.4575306418207306, 2.451266277527106, 4.774246648358827, 7.121206130488806, 25.562099992742088, 11.455301382885398, 2.7539792165229784, 2.680797853987845, 5.040700990570863, 1.141331824743202, 1.138929527568802, 1.145862443068732, 1.0785974298528187, 1.144255219478785, 1.0902369128350464, 1.1390306187181922, 1.0874770150955386, 1.090325547362902, 1.0759700585590326, 1.0937047374272661, 1.0834373993746358, 1.150435504064978, 1.0871696911991222, 1.0891622234513958, 1.2189384294438796, 1.1464103658010694, 1.0860492955719447, 1.0765699587911233, 1.0898651776154222, 1.1449716149166635, 1.14516557304581, 1.0773259557650563, 1.083272807346496, 1.1342576089556455, 1.218243897246931, 1.091904527079804, 1.1380133299496957, 1.1551609409433898, 1.1614231388319605, 1.1516462172982311, 1.2912920684185383, 1.2917606371165942, 1.3009430073955366, 1.2952177699692466, 1.3009289247942804, 1.2140482674118753, 1.2848463804341075, 1.2868968769822375, 1.2949906975809646, 1.2985332164338486, 1.2826563791929593, 1.2787740954125393, 1.8282490663905457, 1.9752818177928708, 1.290621321377978, 1.3012496502782191, 7.121206130488806, 7.134265910900587, 6.925325700549233, 2.1539324811180487, 5.478161993275263, 11.455301382885398, 1.9723429977125366, 4.037902460634305, 5.040700990570863, 7.6036876650673735, 5.026465271384701, 9.900323266197885, 2.1335253169729076, 2.5172428668307334, 25.562099992742088, 2.065576464846706, 5.1882027410266165, 4.436371875857243, 1.0874770150955386, 1.0785974298528187, 1.0834373993746358, 1.0860492955719447, 1.0937047374272661, 1.1342576089556455, 1.0891622234513958, 1.1464103658010694, 1.0773259557650563, 1.0898651776154222, 1.0796843842421726, 1.083314871144308, 1.0759700585590326, 1.1614465656184392, 1.090325547362902, 1.0765699587911233, 1.1548805061813392, 1.1435392640738047, 1.0831414672717605, 1.073610167312813, 1.1390306187181922, 1.136021850436056, 1.1538138923420893, 1.1371474528686956, 1.1548855395433018, 1.091904527079804, 1.1434185761306448, 1.2140482674118753, 1.138929527568802, 1.142178129581756, 1.2347467097881206, 1.1614231388319605, 1.3074311068399456, 1.2236197692762307, 1.2267164804459703, 1.2268483098099259, 1.300550826376602, 1.297319330598499, 1.27575346083862, 1.2335335440674773, 1.2939639518226178, 1.2932402387835575, 1.2917606371165942, 1.2211843308146137, 1.2955667318336197, 1.297534687452322, 1.2261528930553935, 1.2339030632948556, 2.7539792165229784, 1.2949906975809646, 3.326019721929552, 2.064910045910333, 1.9323235210701928, 2.1335253169729076, 2.0589201783935054, 2.137132552585143, 6.925325700549233, 1.083272807346496, 1.091904527079804, 1.0759700585590326, 1.0891622234513958, 1.0773259557650563, 1.150435504064978, 1.142178129581756, 1.0785974298528187, 1.1438734477965913, 1.0902369128350464, 1.1614231388319605, 1.1390306187181922, 1.150158266055001, 1.141331824743202, 1.0834373993746358, 1.0860492955719447, 1.0796843842421726, 1.0831414672717605, 1.0842071683660128, 1.073610167312813, 1.0898651776154222, 1.0874770150955386, 1.1464103658010694, 1.083314871144308, 1.090325547362902, 1.139417043149201, 1.0937047374272661, 1.144255219478785, 1.1493029291225354, 1.1371474528686956, 1.14516557304581, 1.2158783855588935, 1.2213561512075708, 1.1548855395433018, 1.2261528930553935, 1.2335335440674773, 1.3124053666247273, 1.2826563791929593, 1.2985332164338486, 1.3056216514365167, 1.218243897246931, 1.3009901038145366, 1.3043988083460198, 1.292941040410967, 1.2997103160758772, 1.297534687452322, 1.2917606371165942, 1.2949906975809646, 1.3001547107171625, 1.2932402387835575, 1.3087990137966052, 1.3012496502782191, 2.979216802517183, 1.7039015315181847, 3.326019721929552, 1.9975172170955238, 1.9323235210701928, 1.9848528231372298, 2.7391109246225347, 1.0902369128350464, 1.0796843842421726, 1.0785974298528187, 1.0842071683660128, 1.0831414672717605, 1.0773259557650563, 1.0765699587911233, 1.0898651776154222, 1.091904527079804, 1.1474303617135566, 1.0891622234513958, 1.14516557304581, 1.1434185761306448, 1.1390306187181922, 1.0937047374272661, 1.0759700585590326, 1.0874770150955386, 1.150435504064978, 1.0834373993746358, 1.1342576089556455, 1.142178129581756, 1.1449716149166635, 1.083272807346496, 1.145862443068732, 1.083314871144308, 1.1614231388319605, 1.090325547362902, 1.073610167312813, 1.0860492955719447, 1.1380133299496957, 1.1614465656184392, 1.1464103658010694, 1.1548855395433018, 1.2251056954698654, 1.2241168119563277, 1.1581843417732323, 1.292941040410967, 1.230202732376481, 1.2997103160758772, 1.2826563791929593, 1.305371962419626, 1.303255854070607, 1.301795983080225, 1.2912920684185383, 1.300550826376602, 1.3059144323004865, 1.2949906975809646, 1.2938250568484873, 2.7569664141246797, 1.27575346083862, 1.9848528231372298, 2.7406014446003435, 1.7172662553188502, 1.8376992390969786, 4.436371875857243, 4.037902460634305, 1.0871696911991222, 1.0773259557650563, 1.0765699587911233, 1.083272807346496, 1.073610167312813, 1.0796843842421726, 1.083314871144308, 1.0891622234513958, 1.090325547362902, 1.0937047374272661, 1.136021850436056, 1.0898651776154222, 1.139417043149201, 1.1572829961060411, 1.0874770150955386, 1.0902369128350464, 1.0860492955719447, 1.1529852311510065, 1.0785974298528187, 1.091904527079804, 1.0834373993746358, 1.0842071683660128, 1.1364036473084602, 1.1459016989935062, 1.0831414672717605, 1.1449716149166635, 1.1438734477965913, 1.1614465656184392, 1.138929527568802, 1.141331824743202, 1.2156573405308353, 1.2939639518226178, 1.3012496502782191, 1.297534687452322, 1.2337869792971667, 1.145862443068732, 1.2148416267524236, 1.3124053666247273, 1.278452088205146, 1.2160577454058232, 1.2213561512075708, 1.3087990137966052, 1.3009430073955366, 1.3059144323004865, 1.224030039999781, 1.2971465006531302, 1.2955667318336197, 1.8282490663905457, 1.8257844179985387, 1.2952177699692466, 2.5172428668307334, 2.6727821694978284, 1.3011883966630438, 1.0860492955719447, 1.0902369128350464, 1.0842071683660128, 1.0765699587911233, 1.090325547362902, 1.0796843842421726, 1.0891622234513958, 1.0831414672717605, 1.0874770150955386, 1.073610167312813, 1.083272807346496, 1.0773259557650563, 1.083314871144308, 1.0759700585590326, 1.1390306187181922, 1.0898651776154222, 1.0834373993746358, 1.0785974298528187, 1.144255219478785, 1.1449716149166635, 1.141331824743202, 1.0871696911991222, 1.1342576089556455, 1.1459016989935062, 1.1551609409433898, 1.1371474528686956, 1.1493029291225354, 1.1474303617135566, 1.1529852311510065, 1.1364036473084602, 1.230202732376481, 1.2267164804459703, 1.224030039999781, 1.2274885131507494, 1.3064987277039144, 1.216934545241301, 1.292941040410967, 1.2971465006531302, 1.2177822923349175, 1.211738692104574, 1.2955667318336197, 1.2952177699692466, 1.8282490663905457, 1.3009901038145366, 5.040700990570863, 2.5172428668307334, 2.7391109246225347, 2.137132552585143, 2.7539792165229784, 11.455301382885398], \"Term\": [\"keyword\", \"publication\", \"abstract\", \"study\", \"model\", \"extraction\", \"use\", \"method\", \"academic\", \"generate\", \"text\", \"search\", \"topic\", \"paper\", \"base\", \"discipline\", \"section\", \"propose\", \"result\", \"match\", \"research\", \"word\", \"title\", \"researcher\", \"problem\", \"mixture\", \"generation\", \"quickly\", \"previous\", \"solve\", \"publication\", \"study\", \"section\", \"discipline\", \"use\", \"solve\", \"measure\", \"generation\", \"previous\", \"quickly\", \"search\", \"semantic\", \"experimental\", \"idf\", \"hand\", \"process\", \"data\", \"different\", \"nlp\", \"old\", \"preposition\", \"difficult\", \"precision\", \"remove\", \"database\", \"promise\", \"thousand\", \"world\", \"examine\", \"lack\", \"abstract\", \"academic\", \"problem\", \"researcher\", \"word\", \"conduct\", \"text\", \"method\", \"result\", \"keyword\", \"propose\", \"furthermore\", \"generate\", \"ccg\", \"methodology\", \"focus\", \"analyze\", \"compare\", \"extraction\", \"match\", \"research\", \"unsupervised\", \"graph\", \"new\", \"approach\", \"base\", \"elaborate\", \"training\", \"supervised\", \"task\", \"gather\", \"rely\", \"selectivity\", \"emphasis\", \"article\", \"development\", \"guideline\", \"exist\", \"important\", \"perceptron\", \"cluster\", \"comprehensive\", \"sentence\", \"croatian\", \"rule\", \"systematization\", \"candidate\", \"relate\", \"paper\", \"keyword\", \"generate\", \"present\", \"method\", \"propose\", \"partial\", \"complex\", \"node\", \"network\", \"academic\", \"select\", \"survey\", \"word\", \"title\", \"addition\", \"abstract\", \"result\", \"mixture\", \"probability\", \"underlying\", \"probabilistic\", \"collection\", \"lda\", \"topic\", \"lsi\", \"turn\", \"variational\", \"context\", \"bayesian\", \"representation\", \"discrete\", \"classification\", \"estimation\", \"hierarchical\", \"modeling\", \"level\", \"empirical\", \"technique\", \"generative\", \"explicit\", \"unigram\", \"item\", \"collaborative\", \"paramet\", \"approximate\", \"latent\", \"filtering\", \"model\", \"set\", \"document\", \"text\", \"algorithm\", \"inference\", \"efficient\", \"baye\", \"finite\", \"comparing\", \"datum\", \"method\", \"base\", \"provide\", \"present\", \"summary\", \"help\", \"save\", \"whichcan\", \"reduce\", \"short\", \"compose\", \"include\", \"prototype\", \"implement\", \"regard\", \"increasing\", \"handle\", \"hasbeen\", \"online\", \"precious\", \"hard\", \"primarytool\", \"wepresent\", \"ofabstract\", \"rapid\", \"time\", \"extract\", \"title\", \"test\", \"information\", \"contain\", \"set\", \"document\", \"paper\", \"model\", \"keyword\", \"abstract\", \"researcher\", \"author\", \"result\", \"empirical\", \"hierarchical\", \"allocation\", \"handle\", \"modeling\", \"online\", \"classification\", \"include\", \"ofabstract\", \"reduce\", \"rapid\", \"implement\", \"corpora\", \"prototype\", \"hasbeen\", \"training\", \"technique\", \"hard\", \"save\", \"primarytool\", \"latent\", \"item\", \"compose\", \"regard\", \"representation\", \"guideline\", \"time\", \"discrete\", \"approximate\", \"unigram\", \"finite\", \"check\", \"engine\", \"university\", \"social\", \"science\", \"important\", \"grade\", \"truthful\", \"nonetheless\", \"usually\", \"produce\", \"promise\", \"lda\", \"datum\", \"thousand\", \"situation\", \"model\", \"publication\", \"use\", \"quickly\", \"study\", \"abstract\", \"unsupervised\", \"topic\", \"result\", \"academic\", \"text\", \"method\", \"generation\", \"mixture\", \"keyword\", \"analyze\", \"generate\", \"search\", \"include\", \"handle\", \"implement\", \"hard\", \"rapid\", \"representation\", \"hasbeen\", \"technique\", \"compose\", \"primarytool\", \"increasing\", \"precious\", \"reduce\", \"baye\", \"ofabstract\", \"save\", \"report\", \"explicit\", \"short\", \"whichcan\", \"classification\", \"variational\", \"efficient\", \"bayesian\", \"comparing\", \"time\", \"generative\", \"important\", \"hierarchical\", \"turn\", \"node\", \"unigram\", \"methodology\", \"special\", \"systematization\", \"task\", \"introduction\", \"data\", \"idf\", \"complex\", \"year\", \"sobiad\", \"engine\", \"gather\", \"process\", \"score\", \"future\", \"weight\", \"researcher\", \"nonetheless\", \"title\", \"addition\", \"information\", \"generation\", \"analysis\", \"previous\", \"use\", \"regard\", \"time\", \"reduce\", \"hasbeen\", \"compose\", \"corpora\", \"turn\", \"handle\", \"estimation\", \"online\", \"unigram\", \"classification\", \"paramet\", \"empirical\", \"implement\", \"hard\", \"increasing\", \"short\", \"wepresent\", \"whichcan\", \"primarytool\", \"include\", \"technique\", \"precious\", \"ofabstract\", \"context\", \"rapid\", \"modeling\", \"dirichlet\", \"bayesian\", \"item\", \"perceptron\", \"relate\", \"comparing\", \"future\", \"complex\", \"ccg\", \"produce\", \"usually\", \"focus\", \"guideline\", \"textrank\", \"employ\", \"prevent\", \"differently\", \"score\", \"engine\", \"nonetheless\", \"database\", \"sobiad\", \"make\", \"situation\", \"discipline\", \"summary\", \"title\", \"approach\", \"information\", \"graph\", \"provide\", \"online\", \"increasing\", \"handle\", \"wepresent\", \"short\", \"compose\", \"save\", \"primarytool\", \"time\", \"filtering\", \"hasbeen\", \"item\", \"generative\", \"classification\", \"rapid\", \"reduce\", \"include\", \"corpora\", \"implement\", \"representation\", \"turn\", \"latent\", \"regard\", \"allocation\", \"precious\", \"unigram\", \"ofabstract\", \"whichcan\", \"hard\", \"discrete\", \"baye\", \"technique\", \"comparing\", \"work\", \"candidate\", \"collaborative\", \"prevent\", \"network\", \"differently\", \"produce\", \"structure\", \"fuzzy\", \"divide\", \"check\", \"introduction\", \"successful\", \"nonetheless\", \"precision\", \"research\", \"idf\", \"graph\", \"problem\", \"help\", \"underlying\", \"search\", \"topic\", \"prototype\", \"compose\", \"save\", \"regard\", \"whichcan\", \"increasing\", \"precious\", \"hasbeen\", \"ofabstract\", \"rapid\", \"variational\", \"primarytool\", \"context\", \"algorithm\", \"include\", \"online\", \"hard\", \"inference\", \"handle\", \"time\", \"implement\", \"wepresent\", \"lsi\", \"infinite\", \"short\", \"latent\", \"estimation\", \"baye\", \"hierarchical\", \"empirical\", \"development\", \"year\", \"situation\", \"score\", \"select\", \"allocation\", \"rule\", \"ccg\", \"different\", \"cluster\", \"relate\", \"make\", \"university\", \"successful\", \"sentence\", \"turkey\", \"process\", \"lda\", \"probability\", \"social\", \"mixture\", \"present\", \"conduct\", \"hard\", \"online\", \"wepresent\", \"save\", \"ofabstract\", \"increasing\", \"hasbeen\", \"short\", \"include\", \"whichcan\", \"regard\", \"compose\", \"precious\", \"reduce\", \"classification\", \"primarytool\", \"implement\", \"handle\", \"modeling\", \"latent\", \"empirical\", \"prototype\", \"representation\", \"infinite\", \"approximate\", \"bayesian\", \"dirichlet\", \"filtering\", \"inference\", \"lsi\", \"network\", \"systematization\", \"sentence\", \"graphbase\", \"derivation\", \"emphasis\", \"prevent\", \"turkey\", \"elaborate\", \"supervised\", \"process\", \"social\", \"lda\", \"textrank\", \"result\", \"mixture\", \"provide\", \"previous\", \"researcher\", \"abstract\"], \"Freq\": [25.0, 7.0, 11.0, 5.0, 7.0, 5.0, 6.0, 9.0, 7.0, 5.0, 5.0, 4.0, 4.0, 4.0, 4.0, 2.0, 2.0, 3.0, 5.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 6.792151996436759, 5.136368155478925, 2.6296766650661425, 2.6387748218222162, 5.966399575830575, 1.7947935467135108, 1.8038387805246925, 1.7992172843440217, 1.8007960144794488, 1.80978426379631, 3.4724641676463843, 0.9659195513170243, 0.9579826309804387, 0.9568441898610951, 0.9707665364235011, 0.9693182180051433, 0.9704416568041113, 0.9556864673086048, 0.9520082776200519, 0.9697946961937165, 0.9673102316803086, 0.9761984311728314, 0.9651497636267198, 0.9650639717231988, 0.9695319195612307, 0.9535817799547259, 0.9620654657174142, 0.9578916900541308, 0.9638244779421427, 0.9642581060899659, 8.476519699435357, 5.116147549823834, 1.7956598762361347, 1.796285513512913, 1.8015797259301571, 0.968046848649065, 2.6413559544611154, 4.298143065786412, 2.62514235977282, 8.491681916487613, 1.8010494198009006, 0.9722711677092393, 1.80362753199349, 0.9722577405011128, 0.9709334318460628, 0.9702378661024129, 0.9727754101510194, 0.9703823757813688, 5.4636996769946515, 2.407799447994579, 2.4110272773689636, 1.6400904454594207, 1.6437260920645396, 1.638651460440625, 1.6468457672784382, 3.167262315671402, 0.8862595140388971, 0.8856116932871095, 0.877926494469536, 0.888219569112019, 0.8837714118318348, 0.8767837072632757, 0.8772426366699487, 0.8800353981141888, 0.879969049981101, 0.8787033288445999, 0.8798860422395127, 0.882542744715086, 0.8758285679637271, 0.8770741675124962, 0.8770406012028146, 0.8842002172226588, 0.8820079443982782, 0.8839006027173367, 0.8747645662294672, 0.8827282970096527, 0.8807001293161763, 0.8781014254483248, 3.1716861917002563, 13.09566653801267, 3.1705606725071163, 1.648253713801221, 4.6927541256954255, 1.6433592940901631, 0.8828143634151077, 0.88380887556576, 0.8840540908956606, 0.8825944584973565, 1.638436225653902, 0.8824550002377155, 0.8807962788254883, 0.8870672531293515, 0.88413708165581, 0.8830531803511947, 0.8827868948861574, 0.882786808194193, 2.155139087480063, 1.4735776745780031, 1.4808073999233298, 1.480334393300074, 1.4794842285188057, 1.4626096832753264, 2.8539002387159726, 0.7995050094962672, 0.7961127165160111, 0.7893427107093096, 0.7906678875596468, 0.788558007586526, 0.78623960825743, 0.7879546383671325, 0.7884584320773553, 0.791542454896309, 0.7871976759316466, 0.7908566706011936, 0.8034556627820622, 0.7882136537108, 0.7916161982081836, 0.7892863187329803, 0.7893284571111712, 0.8010664301963841, 0.7895323369699029, 0.7981452046505831, 0.7915903348556921, 0.7949824031632637, 0.7879377182410106, 0.7892593207217057, 4.8852940000983, 1.481329893200762, 1.4769879744329149, 2.1621696978121387, 0.7938850318600632, 0.7922996035486476, 0.7919423139401471, 0.7932944445469082, 0.790959468911544, 0.7915454530576564, 0.7947344379439205, 0.7974411599095217, 0.7961057561804237, 0.7945777542986763, 0.7915450437361528, 1.3476836837596893, 1.3470805547879188, 0.7250673858728246, 0.7222804810266802, 0.7211492087511893, 0.725256885553174, 0.7196876894403472, 0.7261287998670627, 0.7253112061159376, 0.7222018228834277, 0.7220878904091724, 0.7191882304740035, 0.7169984604000371, 0.7239219554730512, 0.7220431436225098, 0.7170775434484538, 0.7182803254891538, 0.720765991355634, 0.7166545127493226, 0.7187867034332532, 0.7201032288415082, 0.7185081032624472, 1.345795388182296, 1.3533605612584747, 0.7218712972990966, 0.7279387487777743, 0.7233737166712392, 0.7231226011781016, 0.7201338974953192, 1.3534010577636357, 1.9788813298644963, 3.8485896069076566, 1.970468237926951, 0.7244358581898338, 0.7242286378825622, 0.7280632447310289, 0.0026123004999649016, 0.002565406641146744, 0.0025727150389559016, 0.0023911355543569245, 0.002500656998657394, 0.0023766859720488007, 0.0024707324412403028, 0.002358297965772516, 0.002357925641335051, 0.002321943860394191, 0.002358072637825954, 0.002330204098842334, 0.002472933990392383, 0.0023327268429508205, 0.002335179418672325, 0.002613394339090736, 0.002457866368256276, 0.0023258466401361575, 0.002293237493416728, 0.0023094008169827753, 0.0024185906063476947, 0.002416378449069551, 0.0022731221121172758, 0.00228235583777144, 0.0023876362319225435, 0.0025640318393189945, 0.002290059442111666, 0.0023767832101879074, 0.0024069574126540155, 0.0024134089628411928, 0.002383800151317183, 0.002574584696615655, 0.0025698263465097574, 0.0025431006472839044, 0.0025351200706167877, 0.0025202538221438617, 0.002426072714387857, 0.0024705350519911625, 0.0024639420752725644, 0.002468634571966416, 0.0024667863692177483, 0.00245035116678588, 0.0024478947399718997, 0.0026551490562327397, 0.0026936869876381044, 0.002447006986476023, 0.0024447836232380786, 0.0030693927376192216, 0.0029933966574206515, 0.0029276017576716734, 0.0025829077829696715, 0.002830991569477538, 0.003018596776692997, 0.002545648603681989, 0.0026820132861544704, 0.0027090579388125546, 0.002784486592162646, 0.0026879335969170874, 0.0027624089781647684, 0.0025388156557147558, 0.002554701755452222, 0.0027955839926547318, 0.0025227985146501707, 0.002562237068572298, 0.0025309774163327537, 0.0026643920817490223, 0.0025234309638310346, 0.0025347401629951223, 0.002444902021154524, 0.0024561823853784877, 0.002534792173648943, 0.0024150504684472373, 0.002538024447291947, 0.0023726344774084524, 0.0024002240128007165, 0.002373288801461453, 0.002376748925072228, 0.0023562337971615283, 0.002530553320133648, 0.0023712620532073047, 0.002337950997915926, 0.002505397775430708, 0.002451285681248708, 0.0023217686671582537, 0.00228492698010455, 0.002422235994296614, 0.002409983878820164, 0.0024469336781141188, 0.0024082367380946624, 0.002442670529608702, 0.0023070698056890837, 0.0024149558515800603, 0.0025561046406466703, 0.002397205047611109, 0.0024034563864243166, 0.0025851984527531257, 0.0024390071705888998, 0.002670239739558659, 0.002535652460052718, 0.0025138807864603296, 0.002505610625830378, 0.0025544498244307176, 0.002543746869460023, 0.0025236441257131957, 0.002483850256253254, 0.002523908264956935, 0.002518914489022302, 0.0025097384738220667, 0.0024607688075012346, 0.0024890809920635076, 0.0024836925639401377, 0.002455036454794392, 0.0024555796100261392, 0.002677025090001006, 0.0024647261636462875, 0.002570637719258666, 0.0025173746267152697, 0.0024956811723112217, 0.002502850089504736, 0.002499658247054566, 0.0024928009659858008, 0.002529985588829544, 0.002487169528295067, 0.0024780891143452903, 0.0024362715723578606, 0.0024609958330716187, 0.0024318250167791825, 0.0025940846851524742, 0.002561178378079843, 0.002398374958984428, 0.002542620096211816, 0.0024152997850734373, 0.0025693295082555513, 0.002504977032050665, 0.0025239309879895727, 0.002492738227297012, 0.002366080626285711, 0.002368845947476971, 0.0023493848422854826, 0.0023533607059283796, 0.002352864205482888, 0.002327973371394654, 0.0023508872435418497, 0.0023368862221193835, 0.0024631626120045886, 0.002325488822158061, 0.0023371380619668274, 0.002440724524074153, 0.0023308595535782004, 0.0024384204468628117, 0.0024478267116799617, 0.0024181459509414554, 0.002415816080544893, 0.002528369942789115, 0.002515243763878107, 0.002425670609600175, 0.002511019218071499, 0.0025174298122875426, 0.0025792481311096324, 0.0025365711766990673, 0.002520168049422888, 0.0025143872285653627, 0.002457906425313286, 0.0025028771503769805, 0.002502850432734963, 0.002487460897937033, 0.002485190443312093, 0.002482103435161542, 0.0024804205820126417, 0.002473437552455418, 0.002473158568598501, 0.0024686593835542394, 0.002470220379627505, 0.0024669999296981385, 0.0025380697679479192, 0.0024914414629277314, 0.0025101893660199816, 0.0024905016358908842, 0.002482089184210013, 0.0024772820796268484, 0.002474178829319257, 0.002557981260984053, 0.002521034950842995, 0.002514742377429258, 0.002487552655689626, 0.0024609807753883934, 0.0024280823617643154, 0.0024083029317283214, 0.0024342070752559775, 0.002426338206981226, 0.0025482255950642872, 0.0024106920277412, 0.0025137145144244954, 0.002501945749848487, 0.0024890947856131013, 0.0023886294859970267, 0.0023449121194630096, 0.002368709125800894, 0.0025035015407461596, 0.002356529612527814, 0.0024513768657459605, 0.0024655965719694614, 0.002470898528003463, 0.0023305508742630008, 0.002464253886965358, 0.002328360348711005, 0.002494534620085008, 0.0023130708257054614, 0.0022767735627194407, 0.002300997440617897, 0.00240881187650566, 0.0024570350502119815, 0.0024223347475475906, 0.002438799901405233, 0.0025461429667430475, 0.002532490347977199, 0.0024369464536565153, 0.0026179713323292203, 0.00249512343551053, 0.002535800353704062, 0.002519916344160881, 0.002532290496831143, 0.0025187435524166614, 0.002513762765661089, 0.002495699095183969, 0.0024989167658311835, 0.0024979927378101794, 0.0024906235656701925, 0.0024775308872524273, 0.0026152198001032517, 0.002472171589871538, 0.0025373475730372523, 0.002525892223819224, 0.0024932485680720777, 0.002493239472740382, 0.002486096309300834, 0.002480392089808438, 0.0025676603543805603, 0.0025060026609673163, 0.002501411275998631, 0.0025134085636589467, 0.0024879447585084693, 0.0024773158134931313, 0.002473667185502871, 0.0024775716779144247, 0.002478705018700464, 0.002460899627644587, 0.0025390191788389306, 0.0024299528010400797, 0.00252203931887284, 0.002540964560832942, 0.0023688122625549657, 0.0023635825829639613, 0.0023446555062778367, 0.0024863577700367126, 0.0023242358680824905, 0.002346610024666559, 0.002326630466035231, 0.002322390161956861, 0.002433059258044622, 0.002449817457718557, 0.0023119598846824776, 0.002432873134148605, 0.0024278691826644704, 0.0024627023090889825, 0.0024143942175956696, 0.00241658578579282, 0.00254750217784938, 0.002657785233925544, 0.0026506947389572533, 0.0026261979959052244, 0.0025238259428170063, 0.002408561246628985, 0.0024788363176460907, 0.0025699640064692575, 0.0025369199029231676, 0.002471466687558611, 0.0024749916408387035, 0.002531528038261687, 0.0025205319573217027, 0.0025209105050787318, 0.002476259086092293, 0.002494725596951748, 0.002487972466396698, 0.00257278584569139, 0.0025247435922359867, 0.0024808933495170165, 0.0025085246971635494, 0.0024959094570511458, 0.0024779058170160658, 0.0025418894761184588, 0.0025377869930337535, 0.0025114390869446297, 0.0024869123837199737, 0.002515548751658875, 0.002454598951831103, 0.0024722100607697797, 0.002457650047965607, 0.0024673643987211907, 0.0024283210526052294, 0.0024396639816944518, 0.002423435447533661, 0.00243645451964435, 0.0024119133448047307, 0.002535417547647314, 0.0024069893113957985, 0.00238646812854136, 0.0023587555219834935, 0.0025019889250970035, 0.0024921567768134588, 0.002482565515468039, 0.00235756003070354, 0.002452610946779747, 0.002476982231184983, 0.002495248847381211, 0.0024558797977608285, 0.0024787582225392554, 0.0024677587185263305, 0.0024648334035237644, 0.002427624490751971, 0.0025275832699835073, 0.0025208824566138534, 0.0025072912702450182, 0.0025049004735831484, 0.0025572515027257165, 0.002493630546352902, 0.002508452374531309, 0.002502050000333698, 0.0024756351780375967, 0.002473659206969262, 0.0024894028799085895, 0.002481449439126273, 0.002501721479248285, 0.002470237847972204, 0.002530358845015637, 0.002501605235293978, 0.002500899538297971, 0.0024936586454961437, 0.002491562185218145, 0.0024828819245005583], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"]}};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el1471406246136879688989902436\", ldavis_el1471406246136879688989902436_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el1471406246136879688989902436\", ldavis_el1471406246136879688989902436_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el1471406246136879688989902436\", ldavis_el1471406246136879688989902436_data);\n            })\n         });\n}\n</script>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Topic-Keyword Matrix\ndf_topic_keywords = pd.DataFrame(best_lda_model.components_)\n\n# Assign Column and Index\ndf_topic_keywords.columns = vectorizer.get_feature_names()\ndf_topic_keywords.index = topicnames\n#print(df_topic_keywords)\n",
      "execution_count": 73,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Show top n keywords for each topic\ndef show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n    keywords = np.array(vectorizer.get_feature_names())\n    topic_keywords = []\n    for topic_weights in lda_model.components_:\n        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n        topic_keywords.append(keywords.take(top_keyword_locs))\n    return topic_keywords\n\ntopic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n\n# Topic - Keywords Dataframe\ndf_topic_keywords = pd.DataFrame(topic_keywords)\ndf_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\ndf_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\ndf_topic_keywords",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 74,
          "data": {
            "text/plain": "             Word 0       Word 1       Word 2          Word 3      Word 4  \\\nTopic 0     prevent     research       online       filtering        work   \nTopic 1     keyword        model     abstract           paper       title   \nTopic 2     keyword     abstract  publication             use       study   \nTopic 3     keyword   extraction       method           paper    generate   \nTopic 4        year    situation        score             lda         ccg   \nTopic 5       model        topic         text         mixture         set   \nTopic 6  researcher  methodology      include            node       title   \nTopic 7       model     abstract  publication             use       study   \nTopic 8  derivation         hard       online  classification      result   \nTopic 9     corpora          ccg      unigram            turn  estimation   \n\n             Word 5           Word 6      Word 7       Word 8       Word 9  \\\nTopic 0       graph      differently   candidate    structure      problem   \nTopic 1     summary             help     extract       result  information   \nTopic 2    academic           method      search         text   discipline   \nTopic 3        base         research       match      present     approach   \nTopic 4   prototype      development   algorithm  variational    different   \nTopic 5  underlying    probabilistic  collection     document  probability   \nTopic 6   important     introduction        data    technique      special   \nTopic 7     keyword         academic      method       result        datum   \nTopic 8     network  systematization  ofabstract    wepresent      prevent   \nTopic 9  discipline          produce  perceptron      paramet      usually   \n\n                Word 10      Word 11       Word 12   Word 13        Word 14  \nTopic 0      increasing      produce         fuzzy    handle         divide  \nTopic 1         include    prototype         short      save     researcher  \nTopic 2         section       result       quickly   measure       generate  \nTopic 3           graph      propose  unsupervised       new       academic  \nTopic 4            make  probability        select   context     successful  \nTopic 5             lda        level       unigram       lsi  collaborative  \nTopic 6  representation    implement          baye       use           year  \nTopic 7            text        topic           lda  training      empirical  \nTopic 8        sentence    graphbase        turkey  modeling            lda  \nTopic 9         complex       relate         focus    future          title  ",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word 0</th>\n      <th>Word 1</th>\n      <th>Word 2</th>\n      <th>Word 3</th>\n      <th>Word 4</th>\n      <th>Word 5</th>\n      <th>Word 6</th>\n      <th>Word 7</th>\n      <th>Word 8</th>\n      <th>Word 9</th>\n      <th>Word 10</th>\n      <th>Word 11</th>\n      <th>Word 12</th>\n      <th>Word 13</th>\n      <th>Word 14</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Topic 0</th>\n      <td>prevent</td>\n      <td>research</td>\n      <td>online</td>\n      <td>filtering</td>\n      <td>work</td>\n      <td>graph</td>\n      <td>differently</td>\n      <td>candidate</td>\n      <td>structure</td>\n      <td>problem</td>\n      <td>increasing</td>\n      <td>produce</td>\n      <td>fuzzy</td>\n      <td>handle</td>\n      <td>divide</td>\n    </tr>\n    <tr>\n      <th>Topic 1</th>\n      <td>keyword</td>\n      <td>model</td>\n      <td>abstract</td>\n      <td>paper</td>\n      <td>title</td>\n      <td>summary</td>\n      <td>help</td>\n      <td>extract</td>\n      <td>result</td>\n      <td>information</td>\n      <td>include</td>\n      <td>prototype</td>\n      <td>short</td>\n      <td>save</td>\n      <td>researcher</td>\n    </tr>\n    <tr>\n      <th>Topic 2</th>\n      <td>keyword</td>\n      <td>abstract</td>\n      <td>publication</td>\n      <td>use</td>\n      <td>study</td>\n      <td>academic</td>\n      <td>method</td>\n      <td>search</td>\n      <td>text</td>\n      <td>discipline</td>\n      <td>section</td>\n      <td>result</td>\n      <td>quickly</td>\n      <td>measure</td>\n      <td>generate</td>\n    </tr>\n    <tr>\n      <th>Topic 3</th>\n      <td>keyword</td>\n      <td>extraction</td>\n      <td>method</td>\n      <td>paper</td>\n      <td>generate</td>\n      <td>base</td>\n      <td>research</td>\n      <td>match</td>\n      <td>present</td>\n      <td>approach</td>\n      <td>graph</td>\n      <td>propose</td>\n      <td>unsupervised</td>\n      <td>new</td>\n      <td>academic</td>\n    </tr>\n    <tr>\n      <th>Topic 4</th>\n      <td>year</td>\n      <td>situation</td>\n      <td>score</td>\n      <td>lda</td>\n      <td>ccg</td>\n      <td>prototype</td>\n      <td>development</td>\n      <td>algorithm</td>\n      <td>variational</td>\n      <td>different</td>\n      <td>make</td>\n      <td>probability</td>\n      <td>select</td>\n      <td>context</td>\n      <td>successful</td>\n    </tr>\n    <tr>\n      <th>Topic 5</th>\n      <td>model</td>\n      <td>topic</td>\n      <td>text</td>\n      <td>mixture</td>\n      <td>set</td>\n      <td>underlying</td>\n      <td>probabilistic</td>\n      <td>collection</td>\n      <td>document</td>\n      <td>probability</td>\n      <td>lda</td>\n      <td>level</td>\n      <td>unigram</td>\n      <td>lsi</td>\n      <td>collaborative</td>\n    </tr>\n    <tr>\n      <th>Topic 6</th>\n      <td>researcher</td>\n      <td>methodology</td>\n      <td>include</td>\n      <td>node</td>\n      <td>title</td>\n      <td>important</td>\n      <td>introduction</td>\n      <td>data</td>\n      <td>technique</td>\n      <td>special</td>\n      <td>representation</td>\n      <td>implement</td>\n      <td>baye</td>\n      <td>use</td>\n      <td>year</td>\n    </tr>\n    <tr>\n      <th>Topic 7</th>\n      <td>model</td>\n      <td>abstract</td>\n      <td>publication</td>\n      <td>use</td>\n      <td>study</td>\n      <td>keyword</td>\n      <td>academic</td>\n      <td>method</td>\n      <td>result</td>\n      <td>datum</td>\n      <td>text</td>\n      <td>topic</td>\n      <td>lda</td>\n      <td>training</td>\n      <td>empirical</td>\n    </tr>\n    <tr>\n      <th>Topic 8</th>\n      <td>derivation</td>\n      <td>hard</td>\n      <td>online</td>\n      <td>classification</td>\n      <td>result</td>\n      <td>network</td>\n      <td>systematization</td>\n      <td>ofabstract</td>\n      <td>wepresent</td>\n      <td>prevent</td>\n      <td>sentence</td>\n      <td>graphbase</td>\n      <td>turkey</td>\n      <td>modeling</td>\n      <td>lda</td>\n    </tr>\n    <tr>\n      <th>Topic 9</th>\n      <td>corpora</td>\n      <td>ccg</td>\n      <td>unigram</td>\n      <td>turn</td>\n      <td>estimation</td>\n      <td>discipline</td>\n      <td>produce</td>\n      <td>perceptron</td>\n      <td>paramet</td>\n      <td>usually</td>\n      <td>complex</td>\n      <td>relate</td>\n      <td>focus</td>\n      <td>future</td>\n      <td>title</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_document_topic.iloc[2][3]",
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 77,
          "data": {
            "text/plain": "0.99"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Define function to predict topic for a given text document.\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\ndef predict_topic(text, nlp=nlp):\n    global sent_to_words\n    global lemmatization\n\n    # Step 1: Clean with simple_preprocess\n    mytext_2 = list(sent_to_words(text))\n\n    # Step 2: Lemmatize\n    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\n    # Step 3: Vectorize transform\n    mytext_4 = vectorizer.transform(mytext_3)\n\n    # Step 4: LDA Transform\n    topic_probability_scores = best_lda_model.transform(mytext_4)\n    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n    return topic, topic_probability_scores\n\n# Predict the topic\nmytext = [\"my favourite topic is image processing\"]\ntopic, prob_scores = predict_topic(text = mytext)\nprint(\" about the topics\")\nprint(topic)\nprint(\" probability scores\")\nprint(prob_scores)",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'list' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-cee8fb13ef67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Predict the topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmytext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"my favourite topic is image processing\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmytext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" about the topics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-cee8fb13ef67>\u001b[0m in \u001b[0;36mpredict_topic\u001b[0;34m(text, nlp)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Step 1: Clean with simple_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmytext_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Step 2: Lemmatize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "list=[]\nlist=df_topic_keywords.iloc[int(df_document_topic.iloc[2][10])].tolist()\nlist",
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 89,
          "data": {
            "text/plain": "['keyword',\n 'extraction',\n 'method',\n 'paper',\n 'generate',\n 'base',\n 'research',\n 'match',\n 'present',\n 'approach',\n 'graph',\n 'propose',\n 'unsupervised',\n 'new',\n 'academic']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install -U nltk\n",
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already up-to-date: nltk in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (3.4.5)\nRequirement already satisfied, skipping upgrade: six in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from nltk) (1.11.0)\n\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install -U rake_nltk",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Collecting rake_nltk\n  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\nRequirement already satisfied, skipping upgrade: nltk in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from rake_nltk) (3.4.5)\nRequirement already satisfied, skipping upgrade: six in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from nltk->rake_nltk) (1.11.0)\nBuilding wheels for collected packages: rake-nltk\n  Building wheel for rake-nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=10089 sha256=4e0c1f8e4fdddc7258c29916589de1cb9f61f4f8b185a92c914a908e96d19262\n  Stored in directory: /home/nbuser/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\nSuccessfully built rake-nltk\nInstalling collected packages: rake-nltk\nSuccessfully installed rake-nltk-1.0.4\n\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def convert(list): \n      \n    # Converting integer list to string list \n    s = [str(i) for i in list] \n      \n    # Join list items using join() \n    res = (\" \".join(s)) \n    \n    return(res) \n  \n\nconvert_list=convert(list)",
      "execution_count": 90,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from rake_nltk import Rake\n\nr = Rake() # Uses stopwords for english from NLTK, and all puntuation characters.\nr.extract_keywords_from_text(convert_list)\n    #r.get_ranked_phrases_with_scores() # To get keyword phrases ranked highest to lowest.\nprint(r.get_word_degrees())\nprint(r.get_word_frequency_distribution())",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": "defaultdict(<function Rake._build_word_co_occurance_graph.<locals>.<lambda> at 0x7fe5b41298c8>, {'method': 15, 'paper': 15, 'base': 15, 'keyword': 15, 'extraction': 15, 'research': 15, 'approach': 15, 'graph': 15, 'unsupervised': 15, 'match': 15, 'present': 15, 'new': 15, 'generate': 15, 'propose': 15, 'academic': 15})\nCounter({'method': 1, 'paper': 1, 'base': 1, 'keyword': 1, 'extraction': 1, 'research': 1, 'approach': 1, 'new': 1, 'unsupervised': 1, 'match': 1, 'present': 1, 'graph': 1, 'generate': 1, 'propose': 1, 'academic': 1})\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "list",
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 92,
          "data": {
            "text/plain": "['keyword',\n 'extraction',\n 'method',\n 'paper',\n 'generate',\n 'base',\n 'research',\n 'match',\n 'present',\n 'approach',\n 'graph',\n 'propose',\n 'unsupervised',\n 'new',\n 'academic']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "r.extract_keywords_from_text(df['abstract'][2])\n    #r.get_ranked_phrases_with_scores() # To get keyword phrases ranked highest to lowest.\n\nprint(r.get_word_degrees())\nprint(r.get_word_frequency_distribution())",
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": "defaultdict(<function Rake._build_word_co_occurance_graph.<locals>.<lambda> at 0x7fe5b47920d0>, {'paper': 7, 'approaches': 3, 'well': 1, 'extraction': 12, 'complex': 2, 'elaborated': 1, 'graph': 5, 'provides': 3, 'supervised': 1, 'unsupervised': 5, 'keyword': 14, 'survey': 1, 'methods': 5, 'gathers': 2, 'graphbased': 2, 'work': 2, 'addition': 1, 'proposed': 1, 'method': 4, 'development': 1, 'existing': 2, 'nodes': 2, 'review': 2, 'based': 6, 'research': 4, 'candidates': 2, 'extracts': 2, 'special': 2, 'related': 2, 'guidelines': 3, 'future': 2, 'croatian': 3, 'presents': 2, 'new': 5, 'emphasis': 2, 'selectivity': 1, 'task': 3, 'network': 2, 'systematization': 1, 'comprehensive': 2})\nCounter({'keyword': 5, 'extraction': 4, 'methods': 3, 'paper': 3, 'approaches': 2, 'graph': 2, 'unsupervised': 2, 'based': 2, 'research': 2, 'new': 2, 'method': 1, 'presents': 1, 'complex': 1, 'candidates': 1, 'emphasis': 1, 'provides': 1, 'supervised': 1, 'survey': 1, 'croatian': 1, 'gathers': 1, 'graphbased': 1, 'work': 1, 'addition': 1, 'proposed': 1, 'development': 1, 'existing': 1, 'nodes': 1, 'review': 1, 'future': 1, 'extracts': 1, 'special': 1, 'related': 1, 'well': 1, 'comprehensive': 1, 'elaborated': 1, 'network': 1, 'selectivity': 1, 'task': 1, 'guidelines': 1, 'systematization': 1})\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def checkKey(dict, key): \n      \n    if key in dict.keys(): \n        print(key,end=\", \")\n        print(\"Present, \", end =\" \") \n        print(\"value =\", dict[key]) \n        return dict[key]\n    else: \n        print(\"Not present\") \n        return 0",
      "execution_count": 94,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_keywords=[]\nfor i in range(0,len(list)):\n    result=checkKey(r.get_word_degrees(),list[i])\n    if(result>2):\n        final_keywords.append(list[i])\n        \n    ",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": "keyword, Present,  value = 14\nextraction, Present,  value = 12\nmethod, Present,  value = 4\npaper, Present,  value = 7\nNot present\nNot present\nresearch, Present,  value = 4\nNot present\nNot present\nNot present\ngraph, Present,  value = 5\nNot present\nunsupervised, Present,  value = 5\nnew, Present,  value = 5\nNot present\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(final_keywords)",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['keyword', 'extraction', 'method', 'paper', 'research', 'graph', 'unsupervised', 'new']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
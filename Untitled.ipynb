{
  "cells": [
    {
      "metadata": {},
      "cell_type": "raw",
      "source": "Paper presents a survey of methods and approaches for keyword extraction task. In addition to the systematization of methods, the paper gathers a comprehensive review of existing research. Related work on keyword extraction is elaborated for supervised and unsupervised methods, with special emphasis on graphbased methods as well as Croatian keyword extraction. Selectivity-based keyword extraction method is proposed as a new unsupervised graph-based keyword extraction method which extracts nodes from a complex network as keyword candidates. The paper provides guidelines for future research and development of new graph-based approaches for keyword extraction tfidf"
    },
    {
      "metadata": {},
      "cell_type": "raw",
      "source": "'''Most topic models, such as latent Dirichlet allocation, rely on the bag-of-words assumption.\nHowever, word order and phrases are often critical to capturing the meaning of text in many text mining tasks.\nThis paper presents topical n-grams, a topic model that discovers topics as well as topical phrases.\nThe probabilistic model generates words in their textual order by, for each word, first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution.\nThus our model can model \"white house\" as 5 a special meaning phrase in the 'politics' topic, but not in the 'real estate' topic.\nSuccessive bigrams form longer phrases.\nWe present experiments showing meaningful phrases and more interpretable topics from the NIPS data and improved information retrieval performance on a TREC collection.'''"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Text Categorization (TC), also known as Text Classification, is the task of automatically\nclassifying a set of text documents into different categories from a predefined set. If a\ndocument belongs to exactly one of the categories, it is a single-label classification task;\notherwise, it is a multi-label classification task. TC uses several tools from Information\nRetrieval (IR) and Machine Learning (ML) and has received much attention in the last years\nfrom both researchers in the academia and industry developers. In this paper, we first\ncategorize the documents using KNN based machine learning approach and then return the\nmost relevant documents."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import re\nimport pandas as pd\nimport numpy as np\nimport nltk",
      "execution_count": 120,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "input_str ='''Text Categorization (TC), also known as Text Classification, is the task of automatically\nclassifying a set of text documents into different categories from a predefined set. If a\ndocument belongs to exactly one of the categories, it is a single-label classification task;\notherwise, it is a multi-label classification task. TC uses several tools from Information\nRetrieval (IR) and Machine Learning (ML) and has received much attention in the last years\nfrom both researchers in the academia and industry developers. In this paper, we first\ncategorize the documents using KNN based machine learning approach and then return the\nmost relevant documents.'''\ninput_str = input_str.lower()\nprint(input_str)",
      "execution_count": 147,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "text categorization (tc), also known as text classification, is the task of automatically\nclassifying a set of text documents into different categories from a predefined set. if a\ndocument belongs to exactly one of the categories, it is a single-label classification task;\notherwise, it is a multi-label classification task. tc uses several tools from information\nretrieval (ir) and machine learning (ml) and has received much attention in the last years\nfrom both researchers in the academia and industry developers. in this paper, we first\ncategorize the documents using knn based machine learning approach and then return the\nmost relevant documents.\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "result = re.sub(r\"\\d+\", \"\", input_str)\nprint(result)",
      "execution_count": 148,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "text categorization (tc), also known as text classification, is the task of automatically\nclassifying a set of text documents into different categories from a predefined set. if a\ndocument belongs to exactly one of the categories, it is a single-label classification task;\notherwise, it is a multi-label classification task. tc uses several tools from information\nretrieval (ir) and machine learning (ml) and has received much attention in the last years\nfrom both researchers in the academia and industry developers. in this paper, we first\ncategorize the documents using knn based machine learning approach and then return the\nmost relevant documents.\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''",
      "execution_count": 149,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "no_punct = \"\"\nfor char in result:\n   if char not in punctuations:\n       no_punct = no_punct + char\n\n# display the unpunctuated string\nprint(no_punct)",
      "execution_count": 150,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "text categorization tc also known as text classification is the task of automatically\nclassifying a set of text documents into different categories from a predefined set if a\ndocument belongs to exactly one of the categories it is a singlelabel classification task\notherwise it is a multilabel classification task tc uses several tools from information\nretrieval ir and machine learning ml and has received much attention in the last years\nfrom both researchers in the academia and industry developers in this paper we first\ncategorize the documents using knn based machine learning approach and then return the\nmost relevant documents\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\nstop_words = set(stopwords.words('english')) \n\nword_tokens = word_tokenize(no_punct) \n\nfiltered_sentence = [w for w in word_tokens if not w in stop_words] \n\nfiltered_sentence = [] \n\nfor w in word_tokens: \n\tif w not in stop_words: \n\t\tfiltered_sentence.append(w) \n\nprint(word_tokens) \nprint(filtered_sentence) \n",
      "execution_count": 151,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "['text', 'categorization', 'tc', 'also', 'known', 'as', 'text', 'classification', 'is', 'the', 'task', 'of', 'automatically', 'classifying', 'a', 'set', 'of', 'text', 'documents', 'into', 'different', 'categories', 'from', 'a', 'predefined', 'set', 'if', 'a', 'document', 'belongs', 'to', 'exactly', 'one', 'of', 'the', 'categories', 'it', 'is', 'a', 'singlelabel', 'classification', 'task', 'otherwise', 'it', 'is', 'a', 'multilabel', 'classification', 'task', 'tc', 'uses', 'several', 'tools', 'from', 'information', 'retrieval', 'ir', 'and', 'machine', 'learning', 'ml', 'and', 'has', 'received', 'much', 'attention', 'in', 'the', 'last', 'years', 'from', 'both', 'researchers', 'in', 'the', 'academia', 'and', 'industry', 'developers', 'in', 'this', 'paper', 'we', 'first', 'categorize', 'the', 'documents', 'using', 'knn', 'based', 'machine', 'learning', 'approach', 'and', 'then', 'return', 'the', 'most', 'relevant', 'documents']\n['text', 'categorization', 'tc', 'also', 'known', 'text', 'classification', 'task', 'automatically', 'classifying', 'set', 'text', 'documents', 'different', 'categories', 'predefined', 'set', 'document', 'belongs', 'exactly', 'one', 'categories', 'singlelabel', 'classification', 'task', 'otherwise', 'multilabel', 'classification', 'task', 'tc', 'uses', 'several', 'tools', 'information', 'retrieval', 'ir', 'machine', 'learning', 'ml', 'received', 'much', 'attention', 'last', 'years', 'researchers', 'academia', 'industry', 'developers', 'paper', 'first', 'categorize', 'documents', 'using', 'knn', 'based', 'machine', 'learning', 'approach', 'return', 'relevant', 'documents']\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "input_text=[\"this is th text one tctst\",\"this is text two one asdf\",\"this text will be remove\"]\nall_ = [x for y in filtered_sentence for x in y.split(' ') ]\na,b = np.unique(all_, return_counts = True)\nto_remove = a[b>1]\noutput_text = [' '.join(np.array(y.split(' '))[~np.isin(y.split(' '), to_remove)]) for y in filtered_sentence]",
      "execution_count": 152,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "output_text=filtered_sentence",
      "execution_count": 153,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "k=''\nfinal=[]\nfor w in output_text: \n\tif w not in k: \n\t\tfinal.append(w) \n",
      "execution_count": 154,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "len(final)",
      "execution_count": 155,
      "outputs": [
        {
          "data": {
            "text/plain": "61"
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "raw",
      "source": "from nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nstemmer= PorterStemmer()\nstemlist=[]\nfor word in final:\n    stemlist.append(stemmer.stem(word))"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from nltk.stem.wordnet import WordNetLemmatizer\nwnl=WordNetLemmatizer()\nstemlist=[]\nfor word in final:\n    stemlist.append(wnl.lemmatize(word))",
      "execution_count": 156,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "stemlist",
      "execution_count": 157,
      "outputs": [
        {
          "data": {
            "text/plain": "['text',\n 'categorization',\n 'tc',\n 'also',\n 'known',\n 'text',\n 'classification',\n 'task',\n 'automatically',\n 'classifying',\n 'set',\n 'text',\n 'document',\n 'different',\n 'category',\n 'predefined',\n 'set',\n 'document',\n 'belongs',\n 'exactly',\n 'one',\n 'category',\n 'singlelabel',\n 'classification',\n 'task',\n 'otherwise',\n 'multilabel',\n 'classification',\n 'task',\n 'tc',\n 'us',\n 'several',\n 'tool',\n 'information',\n 'retrieval',\n 'ir',\n 'machine',\n 'learning',\n 'ml',\n 'received',\n 'much',\n 'attention',\n 'last',\n 'year',\n 'researcher',\n 'academia',\n 'industry',\n 'developer',\n 'paper',\n 'first',\n 'categorize',\n 'document',\n 'using',\n 'knn',\n 'based',\n 'machine',\n 'learning',\n 'approach',\n 'return',\n 'relevant',\n 'document']"
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "finalkey=set(stemlist)",
      "execution_count": 158,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "finalkey",
      "execution_count": 159,
      "outputs": [
        {
          "data": {
            "text/plain": "{'academia',\n 'also',\n 'approach',\n 'attention',\n 'automatically',\n 'based',\n 'belongs',\n 'categorization',\n 'categorize',\n 'category',\n 'classification',\n 'classifying',\n 'developer',\n 'different',\n 'document',\n 'exactly',\n 'first',\n 'industry',\n 'information',\n 'ir',\n 'knn',\n 'known',\n 'last',\n 'learning',\n 'machine',\n 'ml',\n 'much',\n 'multilabel',\n 'one',\n 'otherwise',\n 'paper',\n 'predefined',\n 'received',\n 'relevant',\n 'researcher',\n 'retrieval',\n 'return',\n 'set',\n 'several',\n 'singlelabel',\n 'task',\n 'tc',\n 'text',\n 'tool',\n 'us',\n 'using',\n 'year'}"
          },
          "execution_count": 159,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "finalkey=list(finalkey)\nfinalkey",
      "execution_count": 160,
      "outputs": [
        {
          "data": {
            "text/plain": "['machine',\n 'several',\n 'one',\n 'task',\n 'multilabel',\n 'return',\n 'otherwise',\n 'category',\n 'researcher',\n 'year',\n 'developer',\n 'relevant',\n 'categorization',\n 'us',\n 'paper',\n 'retrieval',\n 'attention',\n 'singlelabel',\n 'different',\n 'ir',\n 'ml',\n 'tool',\n 'based',\n 'classifying',\n 'automatically',\n 'set',\n 'belongs',\n 'industry',\n 'received',\n 'much',\n 'information',\n 'using',\n 'predefined',\n 'text',\n 'approach',\n 'classification',\n 'learning',\n 'first',\n 'academia',\n 'also',\n 'document',\n 'categorize',\n 'known',\n 'exactly',\n 'tc',\n 'knn',\n 'last']"
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "raw",
      "source": "strfinalkey=\"\"\nfor i in finalkey:\n    strfinalkey=strfinalkey+\" \"+i\nstrfinalkey"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Using above first method to create a \n# 2D array \nrows, cols = (len(finalkey), len(finalkey)) \narr = [[0 for i in range(cols)] for j in range(rows)]",
      "execution_count": 161,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "li=[]",
      "execution_count": 162,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def fun(i,j):\n    substr=i+\" \"+j\n    c=input_str.count(substr)\n    if(c!=0):\n        li.append(substr)\n    return c",
      "execution_count": 163,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "l=len(finalkey)\nfor i in range(l):\n    for j in range(l):\n        if i==j:\n            arr[i][j]=0\n        else:\n            c=float(fun(finalkey[i],finalkey[j]))\n            if(c==0):\n                arr[i][j]=0\n            else:\n                arr[i][j]=float(1/c)\n",
      "execution_count": 164,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "li",
      "execution_count": 165,
      "outputs": [
        {
          "data": {
            "text/plain": "['machine learning',\n 'several tool',\n 'relevant document',\n 'based machine',\n 'industry developer',\n 'received much',\n 'much attention',\n 'using knn',\n 'predefined set',\n 'text categorization',\n 'text classification',\n 'text document',\n 'classification task',\n 'learning approach',\n 'also known',\n 'document belongs',\n 'exactly one',\n 'tc us',\n 'knn based',\n 'last year']"
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "li=set(li)",
      "execution_count": 166,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "li=list(li)\nli",
      "execution_count": 167,
      "outputs": [
        {
          "data": {
            "text/plain": "['document belongs',\n 'received much',\n 'several tool',\n 'text document',\n 'text classification',\n 'also known',\n 'predefined set',\n 'much attention',\n 'relevant document',\n 'machine learning',\n 'industry developer',\n 'learning approach',\n 'knn based',\n 'classification task',\n 'last year',\n 'exactly one',\n 'using knn',\n 'text categorization',\n 'tc us',\n 'based machine']"
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df=pd.DataFrame(arr)",
      "execution_count": 168,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df.to_csv(\"dftry.csv\")",
      "execution_count": 169,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Python program for Dijkstra's single \n# source shortest path algorithm. The program is \n# for adjacency matrix representation of the graph \n\n# Library for INT_MAX \ndi=list()\nimport sys \n\nclass Graph(): \n    def __init__(self, vertices): \n        self.V = vertices \n        self.graph = [[0 for column in range(vertices)] \n                      for row in range(vertices)] \n    def printSolution(self, dist): \n        print (\"Vertex \\tDistance from Source\")\n        for node in range(self.V): \n            print (node, \"\\t\", dist[node]) \n\n\t# A utility function to find the vertex with \n\t# minimum distance value, from the set of vertices \n\t# not yet included in shortest path tree \n    def minDistance(self, dist, sptSet):\n        # Initilaize minimum distance for next node \n        min = sys.maxsize\n        # Search not nearest vertex not in the \n        # shortest path tree \n        f=0\n        for v in range(self.V):\n            \n            if dist[v] < min and sptSet[v] == False: \n                f=1\n                min = dist[v] \n                min_index = v\n        if(f==1):\n            return min_index\n        else:\n            return 0\n    # Funtion that implements Dijkstra's single source \n    # shortest path algorithm for a graph represented \n    # using adjacency matrix representation \n    def dijkstra(self, src): \n        dist = [sys.maxsize] * self.V \n        dist[src] = 0\n        sptSet = [False] * self.V \n        for cout in range(self.V): \n            # Pick the minimum distance vertex from \n            # the set of vertices not yet processed. \n            # u is always equal to src in first iteration \n            u = self.minDistance(dist, sptSet) \n            # Put the minimum distance vertex in the \n            # shotest path tree \n            sptSet[u] = True\n            # Update dist value of the adjacent vertices \n            # of the picked vertex only if the current \n            # distance is greater than new distance and \n            # the vertex in not in the shotest path tree \n            for v in range(self.V): \n                if self.graph[u][v] > 0 and sptSet[v] == False and dist[v] > dist[u] + self.graph[u][v]: \n                    dist[v] = dist[u] + self.graph[u][v]\n                if(dist[v]==9223372036854775807):\n                    dist[v]=0\n        di.append(max(dist))\n        #self.printSolution(dist) \n\n# Driver program \ng = Graph(len(arr)) \ng.graph = arr \n#print(g.graph)\nfor i in range(len(arr)):\n    g.dijkstra(i)\nprint(di)\n# This code is contributed by Divyanshu Mehta \n",
      "execution_count": 170,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[0.5, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0, 0, 0, 0, 0, 1.0, 1.0, 1.0, 0, 1.0, 1.0, 1.0, 0, 0.5, 1.0, 0, 0, 1.0, 1.0, 0, 0, 1.0, 1.0, 1.0, 1.0]\n"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "rel=list()\nfor i in range(len(arr)):\n    if di[i]!=0:\n        rel.append(finalkey[i])\nrel",
      "execution_count": 171,
      "outputs": [
        {
          "data": {
            "text/plain": "['machine',\n 'several',\n 'relevant',\n 'based',\n 'industry',\n 'received',\n 'much',\n 'using',\n 'predefined',\n 'text',\n 'classification',\n 'learning',\n 'also',\n 'document',\n 'exactly',\n 'tc',\n 'knn',\n 'last']"
          },
          "execution_count": 171,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
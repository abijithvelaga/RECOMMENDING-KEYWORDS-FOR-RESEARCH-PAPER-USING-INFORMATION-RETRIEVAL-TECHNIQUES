{"title":"KNN based Machine Learning Approach for Text and Document Mining","abstract":"Text Categorization (TC), also known as Text Classification, is the task of automatically classifying a set of text documents into different categories from a predefined set. If a document belongs to exactly one of the categories, it is a single-label classification task; otherwise, it is a multi-label classification task. TC uses several tools from Information Retrieval (IR) and Machine Learning (ML) and has received much attention in the last years from both researchers in the academia and industry developers. In this paper, we first categorize the documents using KNN based machine learning approach and then return the most relevant documents."}
{"title":"Topical N-Grams: Phrase and Topic Discovery, with an Application to Information Retrieval","abstract":"Most topic models, such as latent Dirichlet allocation, rely on the bag-of-words assumption. However, word order and phrases are often critical to capturing the meaning of text in many text mining tasks. This paper presents topical n-grams, a topic model that discovers topics as well as topical phrases. The probabilistic model generates words in their textual order by, for each word, first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Thus our model can model white house as a special meaning phrase in the 'politics' topic, but not in the 'real estate' topic. Successive bigrams form longer phrases. We present experiments showing meaningful phrases and more interpretable topics from the NIPS data and improved information retrieval performance on a TREC collection."}
{"title":"Information extraction from research papers using conditional random fields","abstract":"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This article employs conditional random fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. CRFs provide a principled way for incorporating various local features, external lexicon features and globle layout features.  The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. We make an empirical exploration of several factors, including variations on Gaussian, Laplace and hyperbolic-L1 priors for improved regularization, and several classes of features. Based on CRFs, we further present a novel approach for constraint co-reference information extraction;  i.e., improving extraction performance given that we know some citations refer to the same publication. On a standard benchmark dataset, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs. On four co-reference IE datasets, our system significantly improves extraction performance, with an error rate reduction of 6 to 14%."}
{"title":"LDA-based document models for ad-hoc retrieval","abstract":"Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA),  is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval.  We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency."}
{"title":"A machine learning approach to web page filtering using content and structure analysis","abstract":"As the Web continues to grow, it has become increasingly difficult to search for relevant information using traditional search engines. Topic-specific search engines provide an alternative way to support efficient information retrieval on the Web by providing more precise and customized searching in various domains. However, developers of topic-specific search engines need to address two issues: how to locate relevant documents (URLs) on the Web and how to filter out irrelevant documents from a set of documents collected from the Web. This paper reports our research in addressing the second issue. We propose a machine-learning-based approach that combines Web content analysis and Web structure analysis. We represent each Web page by a set of content-based and link-based features, which can be used as the input for various machine learning algorithms. The proposed approach was implemented using both a feedforward/backpropagation neural network and a support vector machine. Two experiments were designed and conducted to compare the proposed Web-feature approach with two existing Web page filtering methods a keyword-based approach and a lexicon-based approach. The experimental results showed that the proposed approach in general performed better than the benchmark approaches, especially when the number of training documents was small. The proposed approaches can be applied in topic-specific search engine development and other Web applications such as Web content management."}
{"title":"Automatic extraction of titles from general documents using machine learning","abstract":"We propose a machine learning approach to title extraction from general documents. By general documents, we mean documents that can belong to any one of a number of specific genres, including presentations, book chapters, technical papers, brochures, reports, and letters. Previously, methods have been proposed mainly for title extraction from research papers. It has not been clear whether it could be possible to conduct automatic title extraction from general documents. As a case study, we consider extraction from Office including Word and PowerPoint. In our approach, we annotate titles in sample documents (for Word and PowerPoint respectively) and take them as training data, train machine learning models, and perform title extraction using the trained models. Our method is unique in that we mainly utilize formatting information such as font size as features in the models. It turns out that the use of formatting information can lead to quite accurate extraction from general documents. Precision and recall for title extraction from Word is 0.810 and 0.837 respectively, and precision and recall for title extraction from PowerPoint is 0.875 and 0.895 respectively in an experiment on intranet data. Other important new findings in this work include that we can train models in one domain and apply them to another domain, and more surprisingly we can even train models in one language and apply them to another language. Moreover, we can significantly improve search ranking results in do document retrieval by using the extracted titles"}
{"title":"SciPlore Xtract: Extracting Titles from Scientific PDF Documents by Analyzing Style Information (Font Size)","abstract":"Extracting titles from a PDF’s full text is an important task in information retrieval to identify PDFs. Existing approaches apply complicated and expensive (in terms of calculating power) machine learning algorithms such as Support Vector Machines and Conditional Random Fields. In this paper we present a simple rule based heuristic, which considers style information (font size) to identify a PDF’s title. In a first experiment we show that this heuristic delivers better results (77.9% accuracy) than a support vector machine by CiteSeer (69.4% accuracy) in an ‘academic search engine’ scenario and better run times (8:19 minutes vs. 57:26 minutes). "}
{"title":"The Challenges of Data Quality and Data Quality Assessment in the Big Data Era","abstract":"High-quality data are the precondition for analyzing and using big data and for guaranteeing the value of the data. Currently, comprehensive analysis and research of quality standards and quality assessment methods for big data are lacking. First, this paper summarizes reviews of data quality research. Second, this paper analyzes the data characteristics of the big data environment, presents quality challenges faced by big data, and formulates a hierarchical data quality framework from the perspective of data users. This framework consists of big data quality dimensions, quality characteristics, and quality indexes. Finally, on the basis of this framework, this paper constructs a dynamic assessment process for data quality. This process has good expansibility and adaptability and can meet the needs of big data quality assessment. The research results enrich the theoretical scope of big data and lay a solid foundation for the future by establishing an assessment model and studying evaluation algorithms."}
{"title":"BUSINESS INTELLIGENCE AND ANALYTICS: FROM BIG DATA TO BIG IMPACT","abstract":" Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework "}
{"title":"Big data challenge: a data management perspective","abstract":" There is a trend that, virtually everyone, ranging from big Web companies to traditional enterprisers to physical science researchers to social scientists, is either already experiencing or anticipating unprecedented growth in the amount of data available in their world, as well as new opportunities and great untapped value. This paper reviews big data challenges from a data management respective. In particular, we discuss big data diversity, big data reduction, big data integration and cleaning, big data indexing and query, and finally big data analysis and mining. Our survey gives a brief overview about big-data-oriented research and problems. "}
{"title":"Deep learning applications and challenges in big data analytics","abstract":"Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process.  Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning."}
{"title":"A Data-Driven Text Mining and Semantic Network Analysis for Design Information Retrieval","abstract":"With the advent of the big-data era, massive information stored in electronic and digital forms on the internet become valuable resources for knowledge discovery in engineering design. Traditional document retrieval method based on document indexing focuses on retrieving individual documents related to the query, but is incapable of discovering the various associations between individual knowledge concepts. Ontology-based technologies, which can extract the inherent relationships between concepts by using advanced text mining tools, can be applied to improve design information retrieval in the large-scale unstructured textual data environment. However, few of the public available ontology database stands on a design and engineering perspective to establish the relations between knowledge concepts. This paper develops a WordNet focusing on design and engineering associations by integrating the text mining approaches to construct an unsupervised learning ontology network. Subsequent probability and velocity network analysis are applied with different statistical behaviors to evaluate the correlation degree between concepts for design information retrieval. The validation results show that the probability and velocity analysis on our constructed ontology network can help recognize the high related complex design and engineering associations between elements. Finally, an engineering design case study demonstrates the use of our constructed semantic network in real-world project for design relations retrieval."}
{"title":"An IOT by information retrieval approach: Smart lights controlled using WiFi","abstract":"In recent years, the rapid development of Internet of Technology (IOT) makes the intelligent home come true as people expect. The intelligent home system creates the more comfortable, safer, humane and intelligent living environment. It can resolve the problems facing by the people who have busy schedules and get a very less amount of time to spend at home which is increasing rapidly around the world. For the solution of this problem, user can depend on the automated machines and gadgets like smart phones. These smart gadgets are using cloud computing which sends and receives signal o the cloud. The data that is of our use can be fetched by matching some key values using the concept of information retrieval. The key objective of this paper is to create a full-fledged application which could let user to operate the lights of their house from any remote location. The user have a list of options to select which light is to be on and when. The only requirement is to have working wifi at home to which the lights are connected. The is developed in Lua Language by using the Esplorer Integrated Development Environment (IDE). We have also used the micro controller chip ESP 8266 to build our board. "}
{"title":" A note on exploration of IoT generated big data using semantics","abstract":"Welcome to this special issue of the Future Generation Computer Systems (FGCS) journal. The special issue compiles seven technical contributions that significantly advance the state-of-the-art in exploration of Internet of Things (IoT) generated big data using semantic web techniques and technologies. "}
{"title":"Realizing the Internet of Things using information-centric networking","abstract":"Nowadays, the Internet connects more objects than people. These devices generate vast amounts of information. Furthermore, identification technologies - such as Radio Frequency Identification (RFID) - enable the association of information with identifiable objects, not necessarily connected to the Internet. All this information is organized into vertical silos. These silos usually belong to different administrative domains and use their own specific communication protocols. In this paper, we present our vision for a global, all-encompassing Internet of Things (IoT) realized through an integrating architecture relying on information and its identifiers/names. We envision the IoT as the architecture that will interconnect all these silos and will make the information generated by or associated with objects globally accessible. Moreover, we argue that the Information-Centric Networking (ICN) paradigm is the ideal candidate architecture for the realization of that IoT vision. In line with this premise, we propose a research agenda for the realization of a full-fledged ICN-based IoT architecture. "}
{"title":"Use of syntactic context to produce term association lists for text retrieval","abstract":"One aspect of world knowledge essential to information retrieval is knowing when two words are related. Knowing word relatedness allows a system given a user's query terms to retrieve relevant documents not containing those exact terms. Two words can be said to be related if they appear in the same contexts Document co-occurrence gives a measure of word relatedness that has proved to be too rough to be useful. The relatively recent apparition of on-line dictionaries and robust and rapid parsers permits the extraction of finer word contexts from large corpora. In this paper, we will describe such an extraction technique that uses only coarse syntactic analysis and no domain knowledge. This technique produces lists of words related to any work appearing in a corpus. When the closest related terms were used in query expansion of a standard information retrieval testbed, the results were much better than that given by document co-occurence techniques, and slightly better than using unexpanded queries, supporting the contention that semantically similar words were indeed extracted by this technique. "}
{"title":"Research Frontiers in Information Retrieval: Report from the Third Strategic Workshop on Information Retrieval in Lorne (SWIRL 2018)","abstract":"The purpose of the Strategic Workshop in Information Retrieval in Lorne is to explore the long-range issues of the Information Retrieval field, to recognize challenges that are on - or even over - the horizon, to build consensus on some of the key challenges, and to disseminate the resulting information to the research community. The intent is that this description of open problems will help to inspire researchers and graduate students to address the questions, and will provide funding agencies data to focus and coordinate support for information retrieval research. "}
{"title":"Research on Homomorphic Encryption Technology and the Applications of it in IOT","abstract":"Homomorphic encryption is a kind of technology which can operate encrypted data directly without affecting the confidentiality of the encryption systems.Recently,Homomorphic encryption technology has made a great development and breakthrough,and earned more and more attention and favor of society.This paper introduced homomorphic encryption technology in details from the aspects of both theory and application.First,we analysized the homogeneity and safety of RSA,HC,IHC and MRS algorithm,then we discussed the applications of homomorphic encryption technology in the Internet of Things including data processing,information retrieval,copyright protection and privacy protection. "}
{"title":"The Seven Ages of Information Retrieval","abstract":"Vannevar Bush's 1945 article set a goal of fast access to the contents of the world's libraries which looks like it will be achieved by 2010, sixty-five years later. Thus, its history is comparable to that of a person. Information retrieval had its schoolboy phase of research in the 1950s and early 1960s; it then struggled for adoption in the 1970s but has, in the 1980s and 1990s, reached acceptance as free-text search systems are used routinely. The tension between statistical and intellectual content analysis seemed to be moving towards purelyg statistical methods; now, on the Web, manual linking is coming back. As we have learned how to handle text, information retrieval is moving on, to projects in sound and image retrieval, along with electronic provision of much of what is now in libraries. We can look forward to completion of Bush's dream, within a single lifespan."}
{"title":"Intelligent mobile agents for information retrieval and knowledge discovery from distributed data and knowledge sources","abstract":"Tools for selective proactive as well as reactive information retrieval and knowledge discovery constitute some of the key enabling technologies for managing the data overload and translating recent advances in automated data acquisition, digital storage, computers and communications into fundamental advances in decision support, scientific discovery and related applications. The paper describes an implementation of intelligent, customizable mobile software agents for information retrieval and knowledge discovery from distributed data sources. These tools are part of the distributed knowledge network (DKN) toolbox that is being developed at the Iowa State University's Artificial Intelligence Laboratory. Experiments with retrieval of journal paper abstracts demonstrate the feasibility of using machine learning to design mobile intelligent agents for customized information retrieval. A similar approach has been successfully employed for knowledge discovery (using machine learning) from distributed data collections. "}
{"title":"Information Filtering: Overview of Issues, Research and Systems","abstract":"An abundant amount of information is created and delivered over electronic media. Users risk becoming overwhelmed by the flow of information, and they lack adequate tools to help them manage the situation. Information filtering (IF) is one of the methods that is rapidly evolving to manage large information flows. The aim of IF is to expose users to only information that is relevant to them. Many IF systems have been developed in recent years for various application domains. Some examples of filtering applications are: filters for search results on the internet that are employed in the Internet software, personal e-mail filters based on personal profiles, listservers or newsgroups filters for groups or individuals, browser filters that block non-valuable information, filters designed to give children access them only to suitable pages, filters for e-commerce applications that address products and promotions to potential customers only, and many more. The different systems use various methods, concepts, and techniques from diverse research areas like: Information Retrieval, Artificial Intelligence, or Behavioral Science. Various systems cover different scope, have divergent functionality, and various platforms. There are many systems of widely varying philosophies, but all share the goal of automatically directing the most valuable information to users in accordance with their User Model, and of helping them use their limited reading time most optimally. This paper clarifies the difference between IF systems and related systems, such as information retrieval (IR) systems, or Extraction systems. The paper defines a framework to classify IF systems according to several parameters, and illustrates the approach with commercial and academic systems. The paper describes the underlying concepts of IF systems and the techniques that are used to implement them. It discusses methods and measurements that are used for evaluation of IF systems and limitations of the current systems. In the conclusion we present research issues in the Information Filtering research arena, such as user modeling, evaluation standardization and integration with digital libraries and Web repositories. "}
{"title":"Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval","abstract":"Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation."}
{"title":"Deep stacking networks for information retrieval","abstract":"Deep stacking networks (DSN) are a special type of deep model equipped with parallel and scalable learning. We report successful applications of DSN to an information retrieval (IR) task pertaining to relevance prediction for sponsor search after careful regularization methods are incorporated to the previous DSN methods developed for speech and image classification tasks. The DSN-based system significantly outperforms the LambdaRank-based system which represents a recent state-of-the-art for IR in normalized discounted cumulative gain (NDCG) measures, despite the use of mean square error as DSN's training objective. We demonstrate desirable monotonic correlation between NDCG and classification rate in a wide range of IR quality. The weaker correlation and more flat relationship in the high IR-quality region suggest the need for developing new learning objectives and optimization methods. "}
{"title":"Catalytic assessment: understanding how MCQs and EVS can foster deep learning","abstract":"One technology for education whose adoption is currently expanding rapidly in UK higher education is that of electronic voting systems (EVS). As with all educational technology, whether learning benefits are achieved depends not on the technology but on whether an improved teaching method is introduced with it. EVS inherently relies on the multiple choice question (MCQ) format, which many feel is associated with the lowest kind of learning of disconnected facts. This paper, however, discusses several ways in which teaching with MCQs, and so with EVS, has transcended this apparent disadvantage, has based itself on deep learning in the sense of focusing on learning relationships between items rather than on recalling disconnected true false items, and so has achieved substantial learning advantages. Six possible learning designs based on MCQs are discussed, and a new function for assessment is identified, namely catalytic assessment, where the purpose of test questions is to trigger subsequent deep learning without direct teaching input. "}
{"title":"An Information Retrieval Approach to Short Text Conversation","abstract":"Human computer conversation is regarded as one of the most difficult problems in artificial intelligence. In this paper, we address one of its key sub-problems, referred to as short text conversation, in which given a message from human, the computer returns a reasonable response to the message. We leverage the vast amount of short conversation data available on social media to study the issue. We propose formalizing short text conversation as a search problem at the first step, and employing state-of-the-art information retrieval (IR) techniques to carry out the task. We investigate the significance as well as the limitation of the IR approach. Our experiments demonstrate that the retrieval-based model can make the system behave rather intelligently, when combined with a huge repository of conversation data from social media."}

